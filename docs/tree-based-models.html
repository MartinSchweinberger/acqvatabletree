<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Part 3 Tree-Based Models | AcqVA Aurora Workshop: From Tables to Trees</title>
  <meta name="description" content="Part 3 Tree-Based Models | AcqVA Aurora Workshop: From Tables to Trees" />
  <meta name="generator" content="bookdown 0.27 and GitBook 2.6.7" />

  <meta property="og:title" content="Part 3 Tree-Based Models | AcqVA Aurora Workshop: From Tables to Trees" />
  <meta property="og:type" content="book" />
  
  
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Part 3 Tree-Based Models | AcqVA Aurora Workshop: From Tables to Trees" />
  
  
  

<meta name="author" content="Martin Schweinberger" />


<meta name="date" content="2022-06-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="working-with-tables.html"/>

<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">From Tables to Trees</a></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i><b>1</b> General Information</a>
<ul>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html#timeline"><i class="fa fa-check"></i>Timeline</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="working-with-tables.html"><a href="working-with-tables.html"><i class="fa fa-check"></i><b>2</b> Working with Tables</a>
<ul>
<li class="chapter" data-level="2.1" data-path="working-with-tables.html"><a href="working-with-tables.html#preparation-and-session-set-up"><i class="fa fa-check"></i><b>2.1</b> Preparation and session set up</a></li>
<li class="chapter" data-level="2.2" data-path="working-with-tables.html"><a href="working-with-tables.html#getting-started"><i class="fa fa-check"></i><b>2.2</b> Getting started</a></li>
<li class="chapter" data-level="2.3" data-path="working-with-tables.html"><a href="working-with-tables.html#loading-tables-into-r"><i class="fa fa-check"></i><b>2.3</b> Loading tables into R</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="working-with-tables.html"><a href="working-with-tables.html#inspecting-tables"><i class="fa fa-check"></i><b>2.4</b> Inspecting tables</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time-1"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="working-with-tables.html"><a href="working-with-tables.html#processing-tabular-data"><i class="fa fa-check"></i><b>2.5</b> Processing tabular data</a></li>
<li class="chapter" data-level="2.6" data-path="working-with-tables.html"><a href="working-with-tables.html#piping"><i class="fa fa-check"></i><b>2.6</b> Piping</a></li>
<li class="chapter" data-level="2.7" data-path="working-with-tables.html"><a href="working-with-tables.html#selecting-and-filtering"><i class="fa fa-check"></i><b>2.7</b> Selecting and filtering</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time-2"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.8" data-path="working-with-tables.html"><a href="working-with-tables.html#changing-data-and-adding-columns"><i class="fa fa-check"></i><b>2.8</b> Changing data and adding columns</a></li>
<li class="chapter" data-level="2.9" data-path="working-with-tables.html"><a href="working-with-tables.html#renaming-columns"><i class="fa fa-check"></i><b>2.9</b> Renaming columns</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time-3"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.10" data-path="working-with-tables.html"><a href="working-with-tables.html#grouping-and-summarising"><i class="fa fa-check"></i><b>2.10</b> Grouping and summarising</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time-4"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.11" data-path="working-with-tables.html"><a href="working-with-tables.html#gathering-and-spreading"><i class="fa fa-check"></i><b>2.11</b> Gathering and Spreading</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time-5"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.12" data-path="working-with-tables.html"><a href="working-with-tables.html#saving-tables-on-your-computer"><i class="fa fa-check"></i><b>2.12</b> Saving tables on your computer</a>
<ul>
<li class="chapter" data-level="" data-path="working-with-tables.html"><a href="working-with-tables.html#excercise-time-6"><i class="fa fa-check"></i>Excercise Time!</a></li>
</ul></li>
<li class="chapter" data-level="2.13" data-path="working-with-tables.html"><a href="working-with-tables.html#outro"><i class="fa fa-check"></i><b>2.13</b> Outro</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="tree-based-models.html"><a href="tree-based-models.html"><i class="fa fa-check"></i><b>3</b> Tree-Based Models</a>
<ul>
<li class="chapter" data-level="3.1" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages"><i class="fa fa-check"></i><b>3.1</b> Advantages</a></li>
<li class="chapter" data-level="3.2" data-path="tree-based-models.html"><a href="tree-based-models.html#problems"><i class="fa fa-check"></i><b>3.2</b> Problems</a></li>
<li class="chapter" data-level="3.3" data-path="tree-based-models.html"><a href="tree-based-models.html#classification-and-regression-trees"><i class="fa fa-check"></i><b>3.3</b> Classification And Regression Trees</a>
<ul>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#splitting-numeric-ordinal-and-true-categorical-variables"><i class="fa fa-check"></i>Splitting numeric, ordinal, and true categorical variables</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="tree-based-models.html"><a href="tree-based-models.html#conditional-inference-trees"><i class="fa fa-check"></i><b>3.4</b> Conditional Inference Trees</a>
<ul>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#prettifying-your-cit-tree"><i class="fa fa-check"></i>Prettifying your CIT tree</a></li>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#problems-of-conditional-inference-trees"><i class="fa fa-check"></i>Problems of Conditional Inference Trees</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests"><i class="fa fa-check"></i><b>3.5</b> Random Forests</a>
<ul>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#bootstrapped-data"><i class="fa fa-check"></i>Bootstrapped Data</a></li>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#out-of-bag-data"><i class="fa fa-check"></i>Out-Of-Bag data</a></li>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#random-variable-selection"><i class="fa fa-check"></i>Random Variable Selection</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="tree-based-models.html"><a href="tree-based-models.html#random-forests-in-r"><i class="fa fa-check"></i><b>3.6</b> Random Forests in R</a></li>
<li class="chapter" data-level="3.7" data-path="tree-based-models.html"><a href="tree-based-models.html#boruta"><i class="fa fa-check"></i><b>3.7</b> Boruta</a>
<ul>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#advantages-1"><i class="fa fa-check"></i>Advantages</a></li>
<li class="chapter" data-level="" data-path="tree-based-models.html"><a href="tree-based-models.html#procedure"><i class="fa fa-check"></i>Procedure</a></li>
<li class="chapter" data-level="3.7.1" data-path="tree-based-models.html"><a href="tree-based-models.html#boruta-in-r"><i class="fa fa-check"></i><b>3.7.1</b> Boruta in R</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="tree-based-models.html"><a href="tree-based-models.html#references"><i class="fa fa-check"></i><b>3.8</b> References</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">AcqVA Aurora Workshop: From Tables to Trees</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tree-based-models" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">Part 3</span> Tree-Based Models<a href="tree-based-models.html#tree-based-models" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><img src="https://slcladal.github.io/images/acqvalab.png" width="100%" /></p>
<p>This tutorial focuses on tree-based models and their implementation in R. For the more advanced, a recommendable resource for tree-based modeling is <span class="citation">Prasad, Iverson, and Liaw (<a href="#ref-prasad2006newer" role="doc-biblioref">2006</a>)</span>, <span class="citation">Strobl, Malley, and Tutz (<a href="#ref-strobl2009tree" role="doc-biblioref">2009</a>)</span> and <span class="citation">Breiman (<a href="#ref-breiman2001modeling" role="doc-biblioref">2001b</a>)</span>. A very good paper dealing with many critical issues related to tree-based models is <span class="citation">Gries (<a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span></p>
<p>Tree-structure models fall into the machine-learning rather than the inference statistics category as they are commonly used for classification and prediction tasks rather than explanation of relationships between variables. The tree structure represents recursive partitioning of the data to minimize residual deviance that is based on iteratively splitting the data into two subsets.</p>
<p>The most basic type of tree-structure model is a decision tree which is a type of classification and regression tree (CART). A more elaborate version of a CART is called a Conditional Inference Tree (CIT). The difference between a CART and a CIT is that CITs use significance tests, e.g. the p-values, to select and split variables rather than some information measures like the Gini coefficient <span class="citation">(<a href="#ref-gries2021statistics" role="doc-biblioref">Gries 2021</a>)</span>.</p>
<p>Like random forests, inference trees are non-parametric and thus do not rely on distributional requirements (or at least on fewer).</p>
<div id="advantages" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Advantages<a href="tree-based-models.html#advantages" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Several advantages have been associated with using tree-based models:</p>
<ol style="list-style-type: decimal">
<li><p>Tree-structure models are very useful because they are <strong>extremely flexible</strong> as they can deal with different types of variables and provide a very good understanding of the structure in the data.</p></li>
<li><p>Tree-structure models have been deemed particularly interesting for linguists because they can handle <strong>moderate sample sizes</strong> and many high-order interactions better then regression models.</p></li>
<li><p>Tree-structure models are (supposedly) better at detecting <strong>non-linear or non-monotonic relationships</strong> between predictors and dependent variables. This also means that they are better at finding and displaying interaction sinvolving many predictors.</p></li>
<li><p>Tree-structure models are <strong>easy</strong> to implement in R and do not require the model selection, validation, and diagnostics associated with regression models.</p></li>
<li><p>Tree-structure models can be used as <strong>variable-selection</strong> procedure which informs about which variables have any sort of significant relationship with the dependent variable and can thereby inform model fitting.</p></li>
</ol>
</div>
<div id="problems" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Problems<a href="tree-based-models.html#problems" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Despite these potential advantages, a word of warning is in order: <span class="citation">Gries (<a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span> admits that tree-based models can be very useful but there are some issues that but some serious short-comings of tree-structure models remain under-explored. For instance,</p>
<ol style="list-style-type: decimal">
<li><p>Forest-models (Random Forests and Boruta) only inform about the <strong>importance</strong> of a variable but not if the variable is important as a main effect or as part of interactions (or both)! The importance only shows that there is some important connection between the predictor and the dependent variable. While partial dependence plots (see <a href="https://christophm.github.io/interpretable-ml-book/pdp.html">here</a> for more information) offer a remedy for this shortcoming to a certain degree, regression models are still much better at dealing with this issue.</p></li>
<li><p>Simple tree-structure models have been shown to fail in detecting the correct predictors if the variance is solely determined by a <strong>single interaction</strong> <span class="citation">(<a href="#ref-gries2021statistics" role="doc-biblioref">Gries 2021, chap. 7.3</a>)</span>. This failure is caused by the fact that the predictor used in the first split of a tree is selected as the one with the strongest main effect <span class="citation">(<a href="#ref-boulesteix2015interaction" role="doc-biblioref">Boulesteix et al. 2015, 344</a>)</span>. This issue can, however, be avoided by hard-coding the interactions as predictors plus using ensemble methods such as random forests rather than individual trees <span class="citation">(see <a href="#ref-gries2021statistics" role="doc-biblioref">Gries 2021, chap. 7.3</a>)</span>.</p></li>
<li><p>Another shortcoming is that tree-structure models partition the data (rather than “fitting a line” through the data which can lead to more <strong>coarse-grained predictions</strong> compared to regression models when dealing with numeric dependent variables <span class="citation">(again, see <a href="#ref-gries2021statistics" role="doc-biblioref">Gries 2021, chap. 7.3</a>)</span>.</p></li>
<li><p><span class="citation">Boulesteix et al. (<a href="#ref-boulesteix2015interaction" role="doc-biblioref">2015, 341</a>)</span> state that high correlations between predictors can hinder the <strong>detection of interactions</strong> when using small data sets. However, regression do not fare better here as they are even more strongly affected by (multi-)collinearity <span class="citation">(see <a href="#ref-gries2021statistics" role="doc-biblioref">Gries 2021, chap. 7.3</a>)</span>.</p></li>
<li><p>Tree-structure models are bad a detecting interactions when the variables have <strong>strong main effects</strong> which is, unfortunately, common when dealing with linguistic data <span class="citation">(<a href="#ref-wrigt2016interac" role="doc-biblioref">Wright, Ziegler, and König 2016</a>)</span>.</p></li>
<li><p>Tree-structure models cannot handle <strong>factorial variables</strong> with many levels (more than 53 levels) which is very common in linguistics where individual speakers or items are variables.</p></li>
<li><p>Forest-models (Random Forests and Boruta) have been deemed to be better at dealing with small data sets. However, this is only because the analysis is based on <strong>permutations</strong> of the original small data set. As such, forest-based models only appear to be better at handling small data sets because they “blow up” the data set rather than really being better at analyzing the original data.</p></li>
</ol>
<p>Before we implement a conditional inference tree in R, we will have a look at how decision trees work. We will do this in more detail here as random forests and Boruta analyses are extensions of inference trees and are therefore based on the same concepts.</p>
<p><strong>Preparation and session set up</strong></p>
<p>This tutorial is based on R. If you have not installed R or are new to it, you will find an introduction to and more information how to use R <a href="https://slcladal.github.io/intror.html">here</a>. For this tutorials, we need to install certain <em>packages</em> from an R <em>library</em> so that the scripts shown below are executed without errors. Before turning to the code below, please install the packages by running the code below this paragraph. If you have already installed the packages mentioned below, then you can skip ahead and ignore this section. To install the necessary packages, simply run the following code - it may take some time (between 1 and 5 minutes to install all of the libraries so you do not need to worry if it takes some time).</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb1-1"><a href="tree-based-models.html#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install packages</span></span>
<span id="cb1-2"><a href="tree-based-models.html#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;Boruta&quot;</span>)</span>
<span id="cb1-3"><a href="tree-based-models.html#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tree&quot;</span>)</span>
<span id="cb1-4"><a href="tree-based-models.html#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;caret&quot;</span>)</span>
<span id="cb1-5"><a href="tree-based-models.html#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;cowplot&quot;</span>)</span>
<span id="cb1-6"><a href="tree-based-models.html#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tidyverse&quot;</span>)</span>
<span id="cb1-7"><a href="tree-based-models.html#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;ggparty&quot;</span>)</span>
<span id="cb1-8"><a href="tree-based-models.html#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;Gmisc&quot;</span>)</span>
<span id="cb1-9"><a href="tree-based-models.html#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;grid&quot;</span>)</span>
<span id="cb1-10"><a href="tree-based-models.html#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;Hmisc&quot;</span>)</span>
<span id="cb1-11"><a href="tree-based-models.html#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;party&quot;</span>)</span>
<span id="cb1-12"><a href="tree-based-models.html#cb1-12" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;partykit&quot;</span>)</span>
<span id="cb1-13"><a href="tree-based-models.html#cb1-13" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;randomForest&quot;</span>)</span>
<span id="cb1-14"><a href="tree-based-models.html#cb1-14" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;pdp&quot;</span>)</span>
<span id="cb1-15"><a href="tree-based-models.html#cb1-15" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;tidyr&quot;</span>)</span>
<span id="cb1-16"><a href="tree-based-models.html#cb1-16" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;RCurl&quot;</span>)</span>
<span id="cb1-17"><a href="tree-based-models.html#cb1-17" aria-hidden="true" tabindex="-1"></a><span class="fu">install.packages</span>(<span class="st">&quot;vip&quot;</span>)</span></code></pre></div>
<p>Now that we have installed the packages, we can activate them as shown below.</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb2-1"><a href="tree-based-models.html#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load packages</span></span>
<span id="cb2-2"><a href="tree-based-models.html#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Boruta)</span>
<span id="cb2-3"><a href="tree-based-models.html#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tree)</span>
<span id="cb2-4"><a href="tree-based-models.html#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb2-5"><a href="tree-based-models.html#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(cowplot)</span>
<span id="cb2-6"><a href="tree-based-models.html#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb2-7"><a href="tree-based-models.html#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggparty)</span>
<span id="cb2-8"><a href="tree-based-models.html#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Gmisc)</span>
<span id="cb2-9"><a href="tree-based-models.html#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(grid)</span>
<span id="cb2-10"><a href="tree-based-models.html#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(Hmisc)</span>
<span id="cb2-11"><a href="tree-based-models.html#cb2-11" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(party)</span>
<span id="cb2-12"><a href="tree-based-models.html#cb2-12" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(partykit)</span>
<span id="cb2-13"><a href="tree-based-models.html#cb2-13" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb2-14"><a href="tree-based-models.html#cb2-14" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)</span>
<span id="cb2-15"><a href="tree-based-models.html#cb2-15" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(RCurl)</span>
<span id="cb2-16"><a href="tree-based-models.html#cb2-16" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyr)</span>
<span id="cb2-17"><a href="tree-based-models.html#cb2-17" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vip)</span></code></pre></div>
<p>Once you have installed R, RStudio, and have also initiated the session by executing the code shown above, you are good to go.</p>
</div>
<div id="classification-and-regression-trees" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Classification And Regression Trees<a href="tree-based-models.html#classification-and-regression-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>The most basic type of tree-structure model is a decision tree which is a type of classification and regression tree (CART). A more elaborate version of a CART is called a Conditional Inference Tree (CIT). The difference between a CART and a CIT is that CITs use significance tests, e.g. the p-values, to select and split variables rather than some information measures like the Gini coefficient <span class="citation">(<a href="#ref-gries2021statistics" role="doc-biblioref">Gries 2021</a>)</span>.</p>
<p>Below is an example of a decision tree which shows what what response to expect - in this case whether a speaker uses discourse <em>like</em> or not. Decision trees, like all CARTs and CITs, answer a simple question, namely <em>How do we best classify elements based on the given predictors?</em>. The answer that decision trees provide is the classification of the elements based on the levels of the predictors. In simple decision trees, all predictors, even those that are not significant are included in the decision tree. The decision tree shows that the best (or most important) predictor for the use of discourse <em>like</em> is age as it is the highest node. Among young speakers, those with high status use <em>like</em> more compared with speakers of lower social status. Among old speakers, women use discourse <em>like</em> more than men.</p>
<p><img src="bookdownproj_files/figure-html/cit1a-1.png" width="672" /></p>
<p>The <em>yes</em> and <em>no</em> at the bottom show if the speaker should be classified as a user of discourse <em>like</em> (<em>yes</em> or <em>no</em>). Each split can be read as <em>true</em> to the left and <em>false</em> to the right. So that, at the first split, if the person is between the ages of 15 and 40, we need to follow the branch to the left while we need to follow to the right if the person is not 15 to 40.</p>
<p>Before going through how this conditional decision tree is generated, let us first go over some basic concepts. The top of the decision tree is called <em>root</em> or <em>root node</em>, the categories at the end of branches are called <em>leaves</em> or <em>leaf nodes</em>. Nodes that are in-between the root and leaves are called <em>internal nodes</em> or just <em>nodes</em>. The root node has only arrows or lines pointing away from it, internal nodes have lines going to and from them, while leaf nodes only have lines pointing towards them.</p>
<p>How to prune and evaluate the accuracy of decision trees is not shown here. If you are interested in this, please check out chapter 7 of <span class="citation">Gries (<a href="#ref-gries2021statistics" role="doc-biblioref">2021</a>)</span> which is a highly recommendable resource that provide a lot of additional information about decision trees and CARTs.</p>
<p>Let us now go over the process by which the decision tree above is generated. In our example, we want to predict whether a person makes use of discourse <em>like</em> given their age, gender, and social status.</p>
<p>In a first step, we load and inspect the data that we will use in this tutorial. As tree-based models require either numeric or factorized data, we factorize the “character” variables in our data.</p>
<div class="sourceCode" id="cb3"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb3-1"><a href="tree-based-models.html#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb3-2"><a href="tree-based-models.html#cb3-2" aria-hidden="true" tabindex="-1"></a>citdata <span class="ot">&lt;-</span> <span class="fu">read.delim</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;treedat.txt&quot;</span>), <span class="at">header =</span> T, <span class="at">sep =</span> <span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>) <span class="sc">%&gt;%</span></span>
<span id="cb3-3"><a href="tree-based-models.html#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="co"># factorize variables (cit require factors instead of character vectors)</span></span>
<span id="cb3-4"><a href="tree-based-models.html#cb3-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.character, factor)</span>
<span id="cb3-5"><a href="tree-based-models.html#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect data</span></span>
<span id="cb3-6"><a href="tree-based-models.html#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(citdata, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##      Age Gender Status LikeUser
## 1  15-40 female   high       no
## 2  15-40 female   high       no
## 3  15-40   male   high       no
## 4  41-80 female    low      yes
## 5  41-80   male   high       no
## 6  41-80   male    low       no
## 7  41-80 female    low      yes
## 8  15-40   male   high       no
## 9  41-80   male    low       no
## 10 41-80   male    low       no</code></pre>
<p>The data now consists of factors with two levels each.</p>
<p>The first step in generating a decision tree consists in determining, what the root of the decision tree should be. This means that we have to determine which of the variables represents the root node. In order to do so, we tabulate for each variable level, how many speakers of that level have used discourse <em>like</em> (LikeUsers) and how many have not used discourse <em>like</em> (NonLikeUsers).</p>
<div class="sourceCode" id="cb5"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb5-1"><a href="tree-based-models.html#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="co"># tabulate data</span></span>
<span id="cb5-2"><a href="tree-based-models.html#cb5-2" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(citdata<span class="sc">$</span>LikeUser, citdata<span class="sc">$</span>Gender)</span></code></pre></div>
<pre><code>##      
##       female male
##   no      43   75
##   yes     91   42</code></pre>
<div class="sourceCode" id="cb7"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb7-1"><a href="tree-based-models.html#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(citdata<span class="sc">$</span>LikeUser, citdata<span class="sc">$</span>Age)</span></code></pre></div>
<pre><code>##      
##       15-40 41-80
##   no     34    84
##   yes    92    41</code></pre>
<div class="sourceCode" id="cb9"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb9-1"><a href="tree-based-models.html#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(citdata<span class="sc">$</span>LikeUser, citdata<span class="sc">$</span>Status)</span></code></pre></div>
<pre><code>##      
##       high low
##   no    33  85
##   yes   73  60</code></pre>
<p>None of the predictors is perfect (the predictors are therefore referred to as <em>impure</em>). To determine which variable is the root, we will calculate the degree of “impurity” for each variable - the variable which has the lowest impurity value will be the root.</p>
<p>The most common measure of impurity in the context of conditional inference trees is called <em>Gini</em> (an alternative that is common when generating regression trees is the deviance). The Gini value or gini index was introduced by Corrado Gini as a measure for income inequality. In our case we seek to maximize inequality of distributions of leave nodes which is why the gini index is useful for tree based models. For each level we apply the following equation to determine the gini impurity value:</p>
<p><span class="math display">\[\begin{equation}

G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}

\end{equation}\]</span></p>
<p>For the node for men, this would mean the following:</p>
<p><span class="math display">\[\begin{equation}

G_{men} = 1-(\frac{42} {42+75})^{2} - (\frac{75} {42+75})^{2} = 0.4602235

\end{equation}\]</span></p>
<p>For women, we calculate G or Gini as follows:</p>
<p><span class="math display">\[\begin{equation}

G_{women} = 1-(\frac{91} {91+43})^{2} - (\frac{43} {91+43})^{2} = 0.4358432

\end{equation}\]</span></p>
<p>To calculate the Gini value of Gender, we need to calculate the weighted average leaf node impurity (weighted because the number of speakers is different in each group). We calculate the weighted average leaf node impurity using the equation below.</p>
<p><span class="math display">\[\begin{equation}

G_{Gender} = \frac{N_{men}} {N_{Total}} \times G_{men} +  \frac{N_{women}} {N_{Total}} \times G_{women}

G_{Gender} = \frac{159} {303} \times 0.4602235 +  \frac{144} {303} \times 0.4358432 = 0.4611915

\end{equation}\]</span></p>
<p>We will now perform the gini-calculation for gender (see below).</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb11-1"><a href="tree-based-models.html#cb11-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for men</span></span>
<span id="cb11-2"><a href="tree-based-models.html#cb11-2" aria-hidden="true" tabindex="-1"></a>gini_men <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(<span class="dv">42</span><span class="sc">/</span>(<span class="dv">42</span><span class="sc">+</span><span class="dv">75</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">75</span><span class="sc">/</span>(<span class="dv">42</span><span class="sc">+</span><span class="dv">75</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb11-3"><a href="tree-based-models.html#cb11-3" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for women</span></span>
<span id="cb11-4"><a href="tree-based-models.html#cb11-4" aria-hidden="true" tabindex="-1"></a>gini_women <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(<span class="dv">91</span><span class="sc">/</span>(<span class="dv">91</span><span class="sc">+</span><span class="dv">43</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">43</span><span class="sc">/</span>(<span class="dv">91</span><span class="sc">+</span><span class="dv">43</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb11-5"><a href="tree-based-models.html#cb11-5" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate weighted average of Gini for Gender</span></span>
<span id="cb11-6"><a href="tree-based-models.html#cb11-6" aria-hidden="true" tabindex="-1"></a>gini_gender <span class="ot">&lt;-</span> <span class="dv">42</span><span class="sc">/</span>(<span class="dv">42</span><span class="sc">+</span><span class="dv">75</span>)<span class="sc">*</span> gini_men <span class="sc">+</span>  <span class="dv">91</span><span class="sc">/</span>(<span class="dv">91</span><span class="sc">+</span><span class="dv">43</span>) <span class="sc">*</span> gini_women</span>
<span id="cb11-7"><a href="tree-based-models.html#cb11-7" aria-hidden="true" tabindex="-1"></a>gini_gender</span></code></pre></div>
<pre><code>## [1] 0.4611915</code></pre>
<p>The gini for gender is 0.4612. In a next step, we revisit the age distribution and we continue to calculate the gini value for age.</p>
<div class="sourceCode" id="cb13"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb13-1"><a href="tree-based-models.html#cb13-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for age groups</span></span>
<span id="cb13-2"><a href="tree-based-models.html#cb13-2" aria-hidden="true" tabindex="-1"></a>gini_young <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(<span class="dv">92</span><span class="sc">/</span>(<span class="dv">92</span><span class="sc">+</span><span class="dv">34</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">34</span><span class="sc">/</span>(<span class="dv">92</span><span class="sc">+</span><span class="dv">34</span>))<span class="sc">^</span><span class="dv">2</span>  <span class="co"># Gini: young</span></span>
<span id="cb13-3"><a href="tree-based-models.html#cb13-3" aria-hidden="true" tabindex="-1"></a>gini_old <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(<span class="dv">41</span><span class="sc">/</span>(<span class="dv">41</span><span class="sc">+</span><span class="dv">84</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">84</span><span class="sc">/</span>(<span class="dv">41</span><span class="sc">+</span><span class="dv">84</span>))<span class="sc">^</span><span class="dv">2</span>    <span class="co"># Gini: old</span></span>
<span id="cb13-4"><a href="tree-based-models.html#cb13-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate weighted average of Gini for Age</span></span>
<span id="cb13-5"><a href="tree-based-models.html#cb13-5" aria-hidden="true" tabindex="-1"></a>gini_age <span class="ot">&lt;-</span> <span class="dv">92</span><span class="sc">/</span>(<span class="dv">92</span><span class="sc">+</span><span class="dv">34</span>)<span class="sc">*</span> gini_young <span class="sc">+</span>  <span class="dv">41</span><span class="sc">/</span>(<span class="dv">41</span><span class="sc">+</span><span class="dv">84</span>) <span class="sc">*</span> gini_old</span>
<span id="cb13-6"><a href="tree-based-models.html#cb13-6" aria-hidden="true" tabindex="-1"></a>gini_age</span></code></pre></div>
<pre><code>## [1] 0.4323148</code></pre>
<p>The gini for age is .4323 and we continue by revisiting the status distribution and we continue to calculate the gini value for status.</p>
<div class="sourceCode" id="cb15"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb15-1"><a href="tree-based-models.html#cb15-1" aria-hidden="true" tabindex="-1"></a>gini_high <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(<span class="dv">73</span><span class="sc">/</span>(<span class="dv">33</span><span class="sc">+</span><span class="dv">73</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">33</span><span class="sc">/</span>(<span class="dv">33</span><span class="sc">+</span><span class="dv">73</span>))<span class="sc">^</span><span class="dv">2</span>   <span class="co"># Gini: high</span></span>
<span id="cb15-2"><a href="tree-based-models.html#cb15-2" aria-hidden="true" tabindex="-1"></a>gini_low <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(<span class="dv">60</span><span class="sc">/</span>(<span class="dv">60</span><span class="sc">+</span><span class="dv">85</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">85</span><span class="sc">/</span>(<span class="dv">60</span><span class="sc">+</span><span class="dv">85</span>))<span class="sc">^</span><span class="dv">2</span>    <span class="co"># Gini: low</span></span>
<span id="cb15-3"><a href="tree-based-models.html#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate weighted average of Gini for Status</span></span>
<span id="cb15-4"><a href="tree-based-models.html#cb15-4" aria-hidden="true" tabindex="-1"></a>gini_status <span class="ot">&lt;-</span> <span class="dv">73</span><span class="sc">/</span>(<span class="dv">33</span><span class="sc">+</span><span class="dv">73</span>)<span class="sc">*</span> gini_high <span class="sc">+</span>  <span class="dv">60</span><span class="sc">/</span>(<span class="dv">60</span><span class="sc">+</span><span class="dv">85</span>) <span class="sc">*</span> gini_low</span>
<span id="cb15-5"><a href="tree-based-models.html#cb15-5" aria-hidden="true" tabindex="-1"></a>gini_status</span></code></pre></div>
<pre><code>## [1] 0.4960521</code></pre>
<p>The gini for status is .4961 and we can now compare the gini values for age, gender, and status.</p>
<div class="sourceCode" id="cb17"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb17-1"><a href="tree-based-models.html#cb17-1" aria-hidden="true" tabindex="-1"></a><span class="co"># compare age, gender, and status ginis</span></span>
<span id="cb17-2"><a href="tree-based-models.html#cb17-2" aria-hidden="true" tabindex="-1"></a>gini_age; gini_gender; gini_status</span></code></pre></div>
<pre><code>## [1] 0.4323148</code></pre>
<pre><code>## [1] 0.4611915</code></pre>
<pre><code>## [1] 0.4960521</code></pre>
<p>Since age has the lowest gini (impurity) value, our first split is by age and age, thus, represents our root node. Our manually calculated conditional inference tree right now looks as below.</p>
<p><img src="bookdownproj_files/figure-html/cit13-1.png" width="672" /></p>
<p>In a next step, we need to find out which of the remaining variables best separates the speakers who use discourse <em>like</em> from those that do not under the first node. In order to do so, we calculate the Gini values for <em>Gender</em> and <em>SocialStatus</em> for the <em>15-40</em> node.</p>
<p>We thus move on and test if and how to split this branch.</p>
<div class="sourceCode" id="cb21"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb21-1"><a href="tree-based-models.html#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 5TH NODE</span></span>
<span id="cb21-2"><a href="tree-based-models.html#cb21-2" aria-hidden="true" tabindex="-1"></a><span class="co"># split data according to first split (only young data)</span></span>
<span id="cb21-3"><a href="tree-based-models.html#cb21-3" aria-hidden="true" tabindex="-1"></a>young <span class="ot">&lt;-</span> citdata <span class="sc">%&gt;%</span></span>
<span id="cb21-4"><a href="tree-based-models.html#cb21-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(Age <span class="sc">==</span> <span class="st">&quot;15-40&quot;</span>)</span>
<span id="cb21-5"><a href="tree-based-models.html#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect distribution</span></span>
<span id="cb21-6"><a href="tree-based-models.html#cb21-6" aria-hidden="true" tabindex="-1"></a>tbyounggender <span class="ot">&lt;-</span> <span class="fu">table</span>(young<span class="sc">$</span>LikeUser, young<span class="sc">$</span>Gender)</span>
<span id="cb21-7"><a href="tree-based-models.html#cb21-7" aria-hidden="true" tabindex="-1"></a>tbyounggender</span></code></pre></div>
<pre><code>##      
##       female male
##   no      17   17
##   yes     58   34</code></pre>
<div class="sourceCode" id="cb23"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb23-1"><a href="tree-based-models.html#cb23-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for Gender</span></span>
<span id="cb23-2"><a href="tree-based-models.html#cb23-2" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for men</span></span>
<span id="cb23-3"><a href="tree-based-models.html#cb23-3" aria-hidden="true" tabindex="-1"></a>gini_youngmen <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(tbyounggender[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyounggender[,<span class="dv">2</span>]))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (tbyounggender[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyounggender[,<span class="dv">2</span>]))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-4"><a href="tree-based-models.html#cb23-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for women</span></span>
<span id="cb23-5"><a href="tree-based-models.html#cb23-5" aria-hidden="true" tabindex="-1"></a>gini_youngwomen <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(tbyounggender[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyounggender[,<span class="dv">1</span>]))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (tbyounggender[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyounggender[,<span class="dv">1</span>]))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb23-6"><a href="tree-based-models.html#cb23-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # calculate weighted average of Gini for Gender</span></span>
<span id="cb23-7"><a href="tree-based-models.html#cb23-7" aria-hidden="true" tabindex="-1"></a>gini_younggender <span class="ot">&lt;-</span> <span class="fu">sum</span>(tbyounggender[,<span class="dv">2</span>])<span class="sc">/</span><span class="fu">sum</span>(tbyounggender)<span class="sc">*</span> gini_youngmen <span class="sc">+</span>  <span class="fu">sum</span>(tbyounggender[,<span class="dv">1</span>])<span class="sc">/</span><span class="fu">sum</span>(tbyounggender) <span class="sc">*</span> gini_youngwomen</span>
<span id="cb23-8"><a href="tree-based-models.html#cb23-8" aria-hidden="true" tabindex="-1"></a>gini_younggender</span></code></pre></div>
<pre><code>## [1] 0.3885714</code></pre>
<p>The gini value for gender among young speakers is 0.3886.</p>
<p>We continue by inspecting the status distribution.</p>
<div class="sourceCode" id="cb25"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb25-1"><a href="tree-based-models.html#cb25-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for Status</span></span>
<span id="cb25-2"><a href="tree-based-models.html#cb25-2" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect distribution</span></span>
<span id="cb25-3"><a href="tree-based-models.html#cb25-3" aria-hidden="true" tabindex="-1"></a>tbyoungstatus <span class="ot">&lt;-</span> <span class="fu">table</span>(young<span class="sc">$</span>LikeUser, young<span class="sc">$</span>Status)</span>
<span id="cb25-4"><a href="tree-based-models.html#cb25-4" aria-hidden="true" tabindex="-1"></a>tbyoungstatus</span></code></pre></div>
<pre><code>##      
##       high low
##   no    11  23
##   yes   57  35</code></pre>
<p>We now calculate the gini value for status.</p>
<div class="sourceCode" id="cb27"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb27-1"><a href="tree-based-models.html#cb27-1" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for status</span></span>
<span id="cb27-2"><a href="tree-based-models.html#cb27-2" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for low</span></span>
<span id="cb27-3"><a href="tree-based-models.html#cb27-3" aria-hidden="true" tabindex="-1"></a>gini_younglow <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(tbyoungstatus[<span class="dv">2</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyoungstatus[,<span class="dv">2</span>]))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (tbyoungstatus[<span class="dv">1</span>,<span class="dv">2</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyoungstatus[,<span class="dv">2</span>]))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb27-4"><a href="tree-based-models.html#cb27-4" aria-hidden="true" tabindex="-1"></a><span class="co"># calculate Gini for high</span></span>
<span id="cb27-5"><a href="tree-based-models.html#cb27-5" aria-hidden="true" tabindex="-1"></a>gini_younghigh <span class="ot">&lt;-</span> <span class="dv">1</span><span class="sc">-</span>(tbyoungstatus[<span class="dv">2</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyoungstatus[,<span class="dv">1</span>]))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (tbyoungstatus[<span class="dv">1</span>,<span class="dv">1</span>]<span class="sc">/</span><span class="fu">sum</span>(tbyoungstatus[,<span class="dv">1</span>]))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb27-6"><a href="tree-based-models.html#cb27-6" aria-hidden="true" tabindex="-1"></a><span class="co"># # calculate weighted average of Gini for status</span></span>
<span id="cb27-7"><a href="tree-based-models.html#cb27-7" aria-hidden="true" tabindex="-1"></a>gini_youngstatus <span class="ot">&lt;-</span> <span class="fu">sum</span>(tbyoungstatus[,<span class="dv">2</span>])<span class="sc">/</span><span class="fu">sum</span>(tbyoungstatus)<span class="sc">*</span> gini_younglow <span class="sc">+</span>  <span class="fu">sum</span>(tbyoungstatus[,<span class="dv">1</span>])<span class="sc">/</span><span class="fu">sum</span>(tbyoungstatus) <span class="sc">*</span> gini_younghigh</span>
<span id="cb27-8"><a href="tree-based-models.html#cb27-8" aria-hidden="true" tabindex="-1"></a>gini_youngstatus</span></code></pre></div>
<pre><code>## [1] 0.3666651</code></pre>
<p>Since the gini value for status (0.3667) is lower than the gini value for gender (0.3886), we split by status.</p>
<p>We would continue to calculate the gini values and always split at the lowest gini levels until we reach a leaf node. Then, we would continue doing the same for the remaining branches until the entire data is binned into different leaf nodes.</p>
<p>In addition to plotting the decision tree, we can also check its accuracy. To do so, we predict the use of <em>like</em> based on the decision tree and compare them to the observed uses of <em>like</em>. Then we use the <code>confusionMatrix</code> function from the <code>caret</code> package to get an overview of the accuracy statistics.</p>
<div class="sourceCode" id="cb29"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb29-1"><a href="tree-based-models.html#cb29-1" aria-hidden="true" tabindex="-1"></a>dtreeprediction <span class="ot">&lt;-</span> <span class="fu">as.factor</span>(<span class="fu">ifelse</span>(<span class="fu">predict</span>(dtree)[,<span class="dv">2</span>] <span class="sc">&gt;</span> .<span class="dv">5</span>, <span class="st">&quot;yes&quot;</span>, <span class="st">&quot;no&quot;</span>))</span>
<span id="cb29-2"><a href="tree-based-models.html#cb29-2" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(dtreeprediction, citdata<span class="sc">$</span>LikeUser)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  no yes
##        no   58   8
##        yes  60 125
##                                           
##                Accuracy : 0.7291          
##                  95% CI : (0.6696, 0.7831)
##     No Information Rate : 0.5299          
##     P-Value [Acc &gt; NIR] : 7.873e-11       
##                                           
##                   Kappa : 0.4424          
##                                           
##  Mcnemar&#39;s Test P-Value : 6.224e-10       
##                                           
##             Sensitivity : 0.4915          
##             Specificity : 0.9398          
##          Pos Pred Value : 0.8788          
##          Neg Pred Value : 0.6757          
##              Prevalence : 0.4701          
##          Detection Rate : 0.2311          
##    Detection Prevalence : 0.2629          
##       Balanced Accuracy : 0.7157          
##                                           
##        &#39;Positive&#39; Class : no              
## </code></pre>
<p>The conditional inference tree has an accuracy of 72.9 percent which is significantly better than the base-line accuracy of 53.0 percent (No Information Rate <span class="math inline">\(*\)</span> 100). To understand what the other statistics refer to and how they are calculated, run the command <code>?confusionMatrix</code>.</p>
<div id="splitting-numeric-ordinal-and-true-categorical-variables" class="section level3 unnumbered hasAnchor">
<h3>Splitting numeric, ordinal, and true categorical variables<a href="tree-based-models.html#splitting-numeric-ordinal-and-true-categorical-variables" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>While it is rather straight forward to calculate the Gini values for categorical variables, it may not seem quite as apparent how to calculate splits for numeric or ordinal variables. To illustrate how the algorithm works on such variables, consider the example data set shown below.</p>
<pre><code>##   Age LikeUser
## 1  15      yes
## 2  37       no
## 3  63       no
## 4  42      yes
## 5  22      yes
## 6  27      yes</code></pre>
<p>In a first step, we order the numeric variable so that we arrive at the following table.</p>
<pre><code>##   Age LikeUser
## 1  15      yes
## 2  22      yes
## 3  27      yes
## 4  37       no
## 5  42      yes
## 6  63       no</code></pre>
<p>Next, we calculate the means for each level of “Age”.</p>
<pre><code>##     Age LikeUser
## 1  15.0      yes
## 2  18.5         
## 3  22.0      yes
## 4  24.5         
## 5  27.0      yes
## 6  32.0         
## 7  37.0       no
## 8  39.5         
## 9  42.0      yes
## 10 52.5</code></pre>
<p>Now, we calculate the Gini values for each average level of age. How this is done is shown below for the first split.</p>
<p><span class="math display">\[\begin{equation}

G_{x} = 1 - ( p_{1} )^{2} - ( p_{0} )^{2}

\end{equation}\]</span></p>
<p>For an age smaller than 18.5 this would mean:</p>
<p><span class="math display">\[\begin{equation}

G_{youngerthan18.5} = 1-(\frac{1} {1+0})^{2} - (\frac{0} {1+0})^{2} = 0.0

\end{equation}\]</span></p>
<p>For an age greater than 18.5, we calculate G or Gini as follows:</p>
<p><span class="math display">\[\begin{equation}

G_{olerthan18.5} = 1-(\frac{2} {2+3})^{2} - (\frac{3} {2+3})^{2} = 0.48

\end{equation}\]</span></p>
<p>Now, we calculate the Gini for that split as we have done above.</p>
<p><span class="math display">\[\begin{equation}

G_{split18.5} = \frac{N_{youngerthan18.5}} {N_{Total}} \times G_{youngerthan18.5} +  \frac{N_{olderthan18.5}} {N_{Total}} \times G_{olderthan18.5}

G_{split18.5} = \frac{1} {6} \times 0.0 +  \frac{5} {6} \times 0.48 = 0.4

\end{equation}\]</span></p>
<p>We then have to calculate the gini values for all possible age splits which yields the following results:</p>
<div class="sourceCode" id="cb34"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb34-1"><a href="tree-based-models.html#cb34-1" aria-hidden="true" tabindex="-1"></a><span class="co"># 18.5</span></span>
<span id="cb34-2"><a href="tree-based-models.html#cb34-2" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">0</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">0</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">0</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-3"><a href="tree-based-models.html#cb34-3" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">+</span><span class="dv">3</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">3</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">+</span><span class="dv">3</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-4"><a href="tree-based-models.html#cb34-4" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.0</span> <span class="sc">+</span>  <span class="dv">5</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.48</span></span>
<span id="cb34-5"><a href="tree-based-models.html#cb34-5" aria-hidden="true" tabindex="-1"></a><span class="co"># 24.4</span></span>
<span id="cb34-6"><a href="tree-based-models.html#cb34-6" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">2</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">+</span><span class="dv">0</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">0</span><span class="sc">/</span>(<span class="dv">2</span><span class="sc">+</span><span class="dv">0</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-7"><a href="tree-based-models.html#cb34-7" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">3</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">2</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-8"><a href="tree-based-models.html#cb34-8" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.0</span> <span class="sc">+</span>  <span class="dv">4</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.1875</span></span>
<span id="cb34-9"><a href="tree-based-models.html#cb34-9" aria-hidden="true" tabindex="-1"></a><span class="co"># 32</span></span>
<span id="cb34-10"><a href="tree-based-models.html#cb34-10" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">3</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">+</span><span class="dv">0</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">0</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">+</span><span class="dv">0</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-11"><a href="tree-based-models.html#cb34-11" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">2</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">2</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">2</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-12"><a href="tree-based-models.html#cb34-12" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.0</span> <span class="sc">+</span>  <span class="dv">3</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.4444444</span></span>
<span id="cb34-13"><a href="tree-based-models.html#cb34-13" aria-hidden="true" tabindex="-1"></a><span class="co"># 39.5</span></span>
<span id="cb34-14"><a href="tree-based-models.html#cb34-14" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">3</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>(<span class="dv">3</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-15"><a href="tree-based-models.html#cb34-15" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>(<span class="dv">1</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-16"><a href="tree-based-models.html#cb34-16" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.375</span> <span class="sc">+</span>  <span class="dv">2</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.5</span></span>
<span id="cb34-17"><a href="tree-based-models.html#cb34-17" aria-hidden="true" tabindex="-1"></a><span class="co"># 52.5</span></span>
<span id="cb34-18"><a href="tree-based-models.html#cb34-18" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">4</span><span class="sc">/</span>(<span class="dv">4</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>(<span class="dv">4</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-19"><a href="tree-based-models.html#cb34-19" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span><span class="sc">-</span>(<span class="dv">0</span><span class="sc">/</span>(<span class="dv">0</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span> <span class="sc">-</span> (<span class="dv">1</span><span class="sc">/</span>(<span class="dv">0</span><span class="sc">+</span><span class="dv">1</span>))<span class="sc">^</span><span class="dv">2</span></span>
<span id="cb34-20"><a href="tree-based-models.html#cb34-20" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.32</span> <span class="sc">+</span>  <span class="dv">1</span><span class="sc">/</span><span class="dv">6</span> <span class="sc">*</span> <span class="fl">0.0</span></span></code></pre></div>
<pre><code>##   AgeSplit  Gini
## 1     18.5 0.400
## 2     24.5 0.500
## 3     32.0 0.444
## 4     39.5 0.410
## 5     52.5 0.267</code></pre>
<p>The split at 52.5 years of age has the lowest Gini value. Accordingly, we would split the data between speakers who are younger than 52.5 and speakers who are older than 52.5 years of age. The lowest Gini value for any age split would also be the Gini value that would be compared to other variables.</p>
<p>The same procedure that we have used to determine potential splits for a numeric variable would apply to an ordinal variable with only two differences:</p>
<ul>
<li>The Gini values are calculated for the actual levels and not the means between variable levels.</li>
<li>The Gini value is nor calculated for the lowest and highest level as the calculation of the Gini values is impossible for extreme values. Extreme levels can, therefore, not serve as a potential split location.</li>
</ul>
<p>When dealing with categorical variables with more than two levels, the situation is slightly more complex as we would also have to calculate the Gini values for combinations of variable levels. While the calculations are, in principle, analogous to the ones performed for binary of nominal categorical variables, we would also have to check if combinations would lead to improved splits. For instance, imagine we have a variable with categories A, B, and C. In such cases we would not only have to calculate the Gini scores for A, B, and C but also for A plus B, A plus C, and B plus C. Note that we ignore the combination A plus B plus C as this combination would include all other potential combinations.</p>
</div>
</div>
<div id="conditional-inference-trees" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Conditional Inference Trees<a href="tree-based-models.html#conditional-inference-trees" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Conditional Inference Trees (CITs) are much better at determining the <em>true</em> effect of a predictor, i.e. the effect of a predictor if all other effects are simultaneously considered. In contrast to CARTs, CITs use p-values to determine splits in the data. Below is a conditional inference tree which shows how and what factors contribute to the use of discourse <em>like</em>. In conditional inference trees predictors are only included if the predictor is significant (i.e. if these predictors are necessary).</p>
<div class="sourceCode" id="cb36"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb36-1"><a href="tree-based-models.html#cb36-1" aria-hidden="true" tabindex="-1"></a>citdata <span class="ot">&lt;-</span> <span class="fu">read.delim</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;treedat.txt&quot;</span>), <span class="at">header =</span> T, <span class="at">sep =</span> <span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>)</span>
<span id="cb36-2"><a href="tree-based-models.html#cb36-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">111</span>)        <span class="co"># set.seed</span></span>
<span id="cb36-3"><a href="tree-based-models.html#cb36-3" aria-hidden="true" tabindex="-1"></a><span class="co"># apply bonferroni correction (1 minus alpha multiplied by n of predictors)</span></span>
<span id="cb36-4"><a href="tree-based-models.html#cb36-4" aria-hidden="true" tabindex="-1"></a>control <span class="ot">=</span> <span class="fu">ctree_control</span>(<span class="at">mincriterion =</span> <span class="dv">1</span><span class="sc">-</span>(.<span class="dv">05</span><span class="sc">*</span><span class="fu">ncol</span>(citdata)<span class="sc">-</span><span class="dv">1</span>))</span>
<span id="cb36-5"><a href="tree-based-models.html#cb36-5" aria-hidden="true" tabindex="-1"></a><span class="co"># convert character strings to factors</span></span>
<span id="cb36-6"><a href="tree-based-models.html#cb36-6" aria-hidden="true" tabindex="-1"></a>citdata <span class="ot">&lt;-</span> citdata <span class="sc">%&gt;%</span></span>
<span id="cb36-7"><a href="tree-based-models.html#cb36-7" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.character, factor)</span>
<span id="cb36-8"><a href="tree-based-models.html#cb36-8" aria-hidden="true" tabindex="-1"></a><span class="co"># create initial conditional inference tree model</span></span>
<span id="cb36-9"><a href="tree-based-models.html#cb36-9" aria-hidden="true" tabindex="-1"></a>citd.ctree <span class="ot">&lt;-</span> partykit<span class="sc">::</span><span class="fu">ctree</span>(LikeUser <span class="sc">~</span> Age <span class="sc">+</span> Gender <span class="sc">+</span> Status,</span>
<span id="cb36-10"><a href="tree-based-models.html#cb36-10" aria-hidden="true" tabindex="-1"></a>                    <span class="at">data =</span> citdata)</span>
<span id="cb36-11"><a href="tree-based-models.html#cb36-11" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(citd.ctree, <span class="at">gp =</span> <span class="fu">gpar</span>(<span class="at">fontsize =</span> <span class="dv">8</span>)) <span class="co"># plot final ctree</span></span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/cit1b-1.png" width="672" /></p>
<div id="prettifying-your-cit-tree" class="section level3 unnumbered hasAnchor">
<h3>Prettifying your CIT tree<a href="tree-based-models.html#prettifying-your-cit-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The easiest and most common way to visualize CITs is to simply use the <code>plot</code> function from <code>base R</code>. However, using this function does not allow to adapt and customize the visualization except for some very basic parameters. The <code>ggparty</code> function allows to use the <code>ggplot</code> syntax to customize CITs which allows more adjustments and is more aesthetically pleasing.</p>
<p>To generate this customized CIT, we activate the <code>ggparty</code> package and extract the significant p-values from the CIT object. We then plot the CIT and define the nodes, edges, and text elements as shown below.</p>
<div class="sourceCode" id="cb37"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb37-1"><a href="tree-based-models.html#cb37-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract p-values</span></span>
<span id="cb37-2"><a href="tree-based-models.html#cb37-2" aria-hidden="true" tabindex="-1"></a>pvals <span class="ot">&lt;-</span> <span class="fu">unlist</span>(<span class="fu">nodeapply</span>(citd.ctree, <span class="at">ids =</span> <span class="fu">nodeids</span>(citd.ctree), <span class="cf">function</span>(n) <span class="fu">info_node</span>(n)<span class="sc">$</span>p.value))</span>
<span id="cb37-3"><a href="tree-based-models.html#cb37-3" aria-hidden="true" tabindex="-1"></a>pvals <span class="ot">&lt;-</span> pvals[pvals <span class="sc">&lt;</span>.<span class="dv">05</span>]</span>
<span id="cb37-4"><a href="tree-based-models.html#cb37-4" aria-hidden="true" tabindex="-1"></a><span class="co"># plotting</span></span>
<span id="cb37-5"><a href="tree-based-models.html#cb37-5" aria-hidden="true" tabindex="-1"></a><span class="fu">ggparty</span>(citd.ctree) <span class="sc">+</span></span>
<span id="cb37-6"><a href="tree-based-models.html#cb37-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_edge</span>() <span class="sc">+</span></span>
<span id="cb37-7"><a href="tree-based-models.html#cb37-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_edge_label</span>() <span class="sc">+</span></span>
<span id="cb37-8"><a href="tree-based-models.html#cb37-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_node_label</span>(<span class="at">line_list =</span> <span class="fu">list</span>(<span class="fu">aes</span>(<span class="at">label =</span> splitvar),</span>
<span id="cb37-9"><a href="tree-based-models.html#cb37-9" aria-hidden="true" tabindex="-1"></a>                                   <span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">paste0</span>(<span class="st">&quot;N=&quot;</span>, nodesize, <span class="st">&quot;, p&quot;</span>, </span>
<span id="cb37-10"><a href="tree-based-models.html#cb37-10" aria-hidden="true" tabindex="-1"></a>                                                      <span class="fu">ifelse</span>(pvals <span class="sc">&lt;</span> .<span class="dv">001</span>, <span class="st">&quot;&lt;.001&quot;</span>, <span class="fu">paste0</span>(<span class="st">&quot;=&quot;</span>, <span class="fu">round</span>(pvals, <span class="dv">3</span>)))), </span>
<span id="cb37-11"><a href="tree-based-models.html#cb37-11" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">size =</span> <span class="dv">10</span>)),</span>
<span id="cb37-12"><a href="tree-based-models.html#cb37-12" aria-hidden="true" tabindex="-1"></a>                  <span class="at">line_gpar =</span> <span class="fu">list</span>(<span class="fu">list</span>(<span class="at">size =</span> <span class="dv">13</span>), </span>
<span id="cb37-13"><a href="tree-based-models.html#cb37-13" aria-hidden="true" tabindex="-1"></a>                                   <span class="fu">list</span>(<span class="at">size =</span> <span class="dv">10</span>)), </span>
<span id="cb37-14"><a href="tree-based-models.html#cb37-14" aria-hidden="true" tabindex="-1"></a>                  <span class="at">ids =</span> <span class="st">&quot;inner&quot;</span>) <span class="sc">+</span></span>
<span id="cb37-15"><a href="tree-based-models.html#cb37-15" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_node_label</span>(<span class="fu">aes</span>(<span class="at">label =</span> <span class="fu">paste0</span>(<span class="st">&quot;Node &quot;</span>, id, <span class="st">&quot;, N = &quot;</span>, nodesize)),</span>
<span id="cb37-16"><a href="tree-based-models.html#cb37-16" aria-hidden="true" tabindex="-1"></a>    <span class="at">ids =</span> <span class="st">&quot;terminal&quot;</span>, <span class="at">nudge_y =</span> <span class="fl">0.01</span>, <span class="at">nudge_x =</span> <span class="fl">0.01</span>) <span class="sc">+</span></span>
<span id="cb37-17"><a href="tree-based-models.html#cb37-17" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_node_plot</span>(<span class="at">gglist =</span> <span class="fu">list</span>(</span>
<span id="cb37-18"><a href="tree-based-models.html#cb37-18" aria-hidden="true" tabindex="-1"></a>    <span class="fu">geom_bar</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">fill =</span> LikeUser),</span>
<span id="cb37-19"><a href="tree-based-models.html#cb37-19" aria-hidden="true" tabindex="-1"></a>             <span class="at">position =</span> <span class="fu">position_dodge</span>(), <span class="at">color =</span> <span class="st">&quot;black&quot;</span>),</span>
<span id="cb37-20"><a href="tree-based-models.html#cb37-20" aria-hidden="true" tabindex="-1"></a>      <span class="fu">theme_minimal</span>(),</span>
<span id="cb37-21"><a href="tree-based-models.html#cb37-21" aria-hidden="true" tabindex="-1"></a>    <span class="fu">theme</span>(<span class="at">panel.grid.major =</span> <span class="fu">element_blank</span>(), </span>
<span id="cb37-22"><a href="tree-based-models.html#cb37-22" aria-hidden="true" tabindex="-1"></a>            <span class="at">panel.grid.minor =</span> <span class="fu">element_blank</span>()),</span>
<span id="cb37-23"><a href="tree-based-models.html#cb37-23" aria-hidden="true" tabindex="-1"></a>      <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;gray50&quot;</span>, <span class="st">&quot;gray80&quot;</span>), <span class="at">guide =</span> <span class="cn">FALSE</span>),</span>
<span id="cb37-24"><a href="tree-based-models.html#cb37-24" aria-hidden="true" tabindex="-1"></a>      <span class="fu">scale_y_continuous</span>(<span class="at">breaks =</span> <span class="fu">seq</span>(<span class="dv">0</span>, <span class="dv">100</span>, <span class="dv">20</span>),</span>
<span id="cb37-25"><a href="tree-based-models.html#cb37-25" aria-hidden="true" tabindex="-1"></a>                         <span class="at">limits =</span> <span class="fu">c</span>(<span class="dv">0</span>, <span class="dv">100</span>)),</span>
<span id="cb37-26"><a href="tree-based-models.html#cb37-26" aria-hidden="true" tabindex="-1"></a>    <span class="fu">xlab</span>(<span class="st">&quot;&quot;</span>), </span>
<span id="cb37-27"><a href="tree-based-models.html#cb37-27" aria-hidden="true" tabindex="-1"></a>    <span class="fu">ylab</span>(<span class="st">&quot;Frequency&quot;</span>),</span>
<span id="cb37-28"><a href="tree-based-models.html#cb37-28" aria-hidden="true" tabindex="-1"></a>      <span class="fu">geom_text</span>(<span class="fu">aes</span>(<span class="at">x =</span> <span class="st">&quot;&quot;</span>, <span class="at">group =</span> LikeUser,</span>
<span id="cb37-29"><a href="tree-based-models.html#cb37-29" aria-hidden="true" tabindex="-1"></a>                    <span class="at">label =</span> <span class="fu">stat</span>(count)),</span>
<span id="cb37-30"><a href="tree-based-models.html#cb37-30" aria-hidden="true" tabindex="-1"></a>                <span class="at">stat =</span> <span class="st">&quot;count&quot;</span>, </span>
<span id="cb37-31"><a href="tree-based-models.html#cb37-31" aria-hidden="true" tabindex="-1"></a>                <span class="at">position =</span> <span class="fu">position_dodge</span>(<span class="fl">0.9</span>), <span class="at">vjust =</span> <span class="sc">-</span><span class="fl">0.7</span>)),</span>
<span id="cb37-32"><a href="tree-based-models.html#cb37-32" aria-hidden="true" tabindex="-1"></a>    <span class="at">shared_axis_labels =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/plotctree-1.png" width="672" /></p>
</div>
<div id="problems-of-conditional-inference-trees" class="section level3 unnumbered hasAnchor">
<h3>Problems of Conditional Inference Trees<a href="tree-based-models.html#problems-of-conditional-inference-trees" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Like other tree-based methods, CITs are very intuitive, multivariate, non-parametric, they do not require large data sets, and they are easy to implement. Despite these obvious advantages, they have at least one major short coming compared to other, more sophisticated tree-structure models (in addition to the general issues that tree-structure models exhibit as discussed above: they are prone to <strong>overfitting</strong> which means that they fit the observed data very well but preform much worse when being applied to new data.</p>
<p>An extension which remedies this problem is to use a so-called ensemble method which grows many varied trees. The most common ensemble method is called a <em>Random Forest Analysis</em> and will have a look at how Random Forests work and how to implement them in R in the next section.</p>
</div>
</div>
<div id="random-forests" class="section level2 hasAnchor" number="3.5">
<h2><span class="header-section-number">3.5</span> Random Forests<a href="tree-based-models.html#random-forests" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Random Forests (RFs) are an extension of Conditional Inference Trees <span class="citation">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001a</a>)</span>. Like Conditional Inference Trees, Random Forests represent a multivariate, non-parametric partitioning method that is particularly useful when dealing with relatively small sample sizes and many predictors (including interactions) and they are insensitive to multicollinearity (if two predictors strongly correlate with the dependent variable AND are highly correlated or collinear, RFs will report both variables as highly important - the ordering in which they were included into the model is irrelevant). The latter point is a real advantage over regression models in particular. Also, RFs outperform CITs in that they are substantially less prone to overfitting and they perform much better when applied to new data. However, random forests have several issues:</p>
<ul>
<li><p>RFs only show variable importance but not if the variable is positively or negatively correlated with the dependent variable;</p></li>
<li><p>RFs do not report if a variable is important as a main effect or as part of an interactions</p></li>
<li><p>RFs do not indicate in which significant interactions a variable is involved.</p></li>
</ul>
<p>Therefore, Random Forest analyses are ideal for classification, imputing missing values, and - though to a lesser degree - as a variable selection procedure but they do not lend themselves for drawing inferences about the relationships of predictors and dependent variables.</p>
<div id="bootstrapped-data" class="section level3 unnumbered hasAnchor">
<h3>Bootstrapped Data<a href="tree-based-models.html#bootstrapped-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forests do not work on one-and-the-same data set (as CITs do) but in Random Forest analyses, many samples (with replacement) are drawn from the original data set. This generation of new data set based on an existing data set is called “bootstrapping”. Bootstrapping allows us to produce many trees based on variations of the original data set rather than dealing with only a single, fixed data set that would produce only a single tree. Therefore, because the data is different each time, the individual CITs are also different.</p>
<p>Imagine, we are dealing with a very small data set to which we want to apply a Random Forest Analysis. The original data set is displayed below.</p>
<pre><code>##    Id   Age Gender Status LikeUser
## 1   1 15-40 female   high       no
## 2   2 15-40 female   high       no
## 3   3 15-40   male   high       no
## 4   4 41-80 female    low      yes
## 5   5 41-80   male   high       no
## 6   6 41-80   male    low       no
## 7   7 41-80 female    low      yes
## 8   8 15-40   male   high       no
## 9   9 41-80   male    low       no
## 10 10 41-80   male    low       no</code></pre>
<p>We now draw a sample from this data set and receive the following data set.</p>
<pre><code>##   Id   Age Gender Status LikeUser
## 1  6 41-80   male    low       no
## 2  3 15-40   male   high       no
## 3  4 41-80 female    low      yes
## 4  1 15-40 female   high       no
## 5  2 15-40 female   high       no
## 6  2 15-40 female   high       no</code></pre>
<p>As you can see, the bootstrapped data contains the second row twice while the fifth row is missing.</p>
</div>
<div id="out-of-bag-data" class="section level3 unnumbered hasAnchor">
<h3>Out-Of-Bag data<a href="tree-based-models.html#out-of-bag-data" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Because the data is reshuffled for every new tree, a part of the data (on average about 30%) remains unused for a given tree. The data that is not used is called Out-Of-Bag data or OOB. The OOB is important because the quality of the overall performance of the random forest can be assessed by applying the resulting tree-model to the data that it was not fit to. The quality of that tree is then measured in the OOB error, which is the error rate of the respective tree if applied to the OOB data.</p>
</div>
<div id="random-variable-selection" class="section level3 unnumbered hasAnchor">
<h3>Random Variable Selection<a href="tree-based-models.html#random-variable-selection" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Random Forests also differ from simple CITs in that at each step, not all possible variables are considered for a node, but only a subset. For example, we have a data set with five predicting independent variables and one dependent variable. When generating a CIT, all possible variables (variables that do not represent a node further up in the tree) are considered as splitting candidates. In Random Forests, only a fixed number (typically the square-root of the number of independent variables) are considered as candidates for a node. So, at each potential split, a fixed number of randomly selected variables is considered potential node candidates.</p>
</div>
</div>
<div id="random-forests-in-r" class="section level2 hasAnchor" number="3.6">
<h2><span class="header-section-number">3.6</span> Random Forests in R<a href="tree-based-models.html#random-forests-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>This section shows how a Random Forest Analysis can be implemented in R. Ina first step, we load and inspect the data.</p>
<div class="sourceCode" id="cb40"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb40-1"><a href="tree-based-models.html#cb40-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load random forest data</span></span>
<span id="cb40-2"><a href="tree-based-models.html#cb40-2" aria-hidden="true" tabindex="-1"></a>rfdata <span class="ot">&lt;-</span> <span class="fu">read.delim</span>(here<span class="sc">::</span><span class="fu">here</span>(<span class="st">&quot;data&quot;</span>, <span class="st">&quot;forestdat.txt&quot;</span>), <span class="at">header =</span> T, <span class="at">sep =</span> <span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>)</span>
<span id="cb40-3"><a href="tree-based-models.html#cb40-3" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect data</span></span>
<span id="cb40-4"><a href="tree-based-models.html#cb40-4" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(rfdata, <span class="dv">10</span>)</span></code></pre></div>
<pre><code>##           ID Gender   Age ConversationType Priming SUFlike
## 1  S1A-061$C  Women Young      MixedGender NoPrime       0
## 2  S1A-023$B  Women Young      MixedGender NoPrime       0
## 3  S1A-054$A  Women Young       SameGender NoPrime       0
## 4  S1A-090$B  Women Young      MixedGender NoPrime       0
## 5  S1A-009$B  Women   Old       SameGender   Prime       0
## 6  S1A-085$E    Men Young      MixedGender   Prime       1
## 7  S1A-003$C  Women Young      MixedGender NoPrime       1
## 8  S1A-084$C  Women Young       SameGender NoPrime       0
## 9  S1A-076$A  Women Young       SameGender NoPrime       0
## 10 S1A-083$D    Men   Old      MixedGender NoPrime       1</code></pre>
<p>The data consists of four categorical variables (<code>Gender</code>, <code>Age</code>, <code>ConversationType</code>, and <code>SUFlike</code>). Our dependent variable is <code>SUFlike</code> which stands for speech-unit final <em>like</em> (a pragmatic marker that is common in Irish English and is used as in <em>A wee girl of her age, like</em>). While Age and Gender are pretty straight forward what they are called, <em>ConversationType</em> encodes whether a conversation has taken place between interlocutors of the same or of different genders.</p>
<p>Before going any further, we need to factorize the variables as tree-based models require factors instead of character variables (but they can, of course, handle numeric and ordinal variables). In addition, we will check if the data contains missing values (NAs; NA stands for <em>not available</em>).</p>
<div class="sourceCode" id="cb42"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb42-1"><a href="tree-based-models.html#cb42-1" aria-hidden="true" tabindex="-1"></a><span class="co"># factorize variables (rf require factors instead of character vectors)</span></span>
<span id="cb42-2"><a href="tree-based-models.html#cb42-2" aria-hidden="true" tabindex="-1"></a>rfdata <span class="ot">&lt;-</span> rfdata <span class="sc">%&gt;%</span></span>
<span id="cb42-3"><a href="tree-based-models.html#cb42-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.character, factor) <span class="sc">%&gt;%</span></span>
<span id="cb42-4"><a href="tree-based-models.html#cb42-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>ID)</span>
<span id="cb42-5"><a href="tree-based-models.html#cb42-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect data</span></span>
<span id="cb42-6"><a href="tree-based-models.html#cb42-6" aria-hidden="true" tabindex="-1"></a>rfdata <span class="sc">%&gt;%</span></span>
<span id="cb42-7"><a href="tree-based-models.html#cb42-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb42-8"><a href="tree-based-models.html#cb42-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">10</span>) </span></code></pre></div>
<pre><code>##    Gender   Age ConversationType Priming SUFlike
## 1   Women Young      MixedGender NoPrime       0
## 2   Women Young      MixedGender NoPrime       0
## 3   Women Young       SameGender NoPrime       0
## 4   Women Young      MixedGender NoPrime       0
## 5   Women   Old       SameGender   Prime       0
## 6     Men Young      MixedGender   Prime       1
## 7   Women Young      MixedGender NoPrime       1
## 8   Women Young       SameGender NoPrime       0
## 9   Women Young       SameGender NoPrime       0
## 10    Men   Old      MixedGender NoPrime       1</code></pre>
<p>We now check if the data contains missing values and remove those (if necessary).</p>
<div class="sourceCode" id="cb44"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb44-1"><a href="tree-based-models.html#cb44-1" aria-hidden="true" tabindex="-1"></a><span class="co"># check for NAs</span></span>
<span id="cb44-2"><a href="tree-based-models.html#cb44-2" aria-hidden="true" tabindex="-1"></a>natest <span class="ot">&lt;-</span> rfdata <span class="sc">%&gt;%</span></span>
<span id="cb44-3"><a href="tree-based-models.html#cb44-3" aria-hidden="true" tabindex="-1"></a>  <span class="fu">na.omit</span>()</span>
<span id="cb44-4"><a href="tree-based-models.html#cb44-4" aria-hidden="true" tabindex="-1"></a><span class="fu">nrow</span>(natest) <span class="co"># no NAs present in data (same number of rows with NAs omitted)</span></span></code></pre></div>
<pre><code>## [1] 2000</code></pre>
<p>We will now generate a first random forest object and inspect its model fit. As random forests rely on re-sampling, we set a seed so that we arrive at the same estimations.</p>
<div class="sourceCode" id="cb46"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb46-1"><a href="tree-based-models.html#cb46-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed</span></span>
<span id="cb46-2"><a href="tree-based-models.html#cb46-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019120204</span>)</span>
<span id="cb46-3"><a href="tree-based-models.html#cb46-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create initial model</span></span>
<span id="cb46-4"><a href="tree-based-models.html#cb46-4" aria-hidden="true" tabindex="-1"></a>rfmodel1 <span class="ot">&lt;-</span> <span class="fu">cforest</span>(SUFlike <span class="sc">~</span> .,  <span class="at">data =</span> rfdata, </span>
<span id="cb46-5"><a href="tree-based-models.html#cb46-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">controls =</span> <span class="fu">cforest_unbiased</span>(<span class="at">ntree =</span> <span class="dv">50</span>, <span class="at">mtry =</span> <span class="dv">3</span>))</span>
<span id="cb46-6"><a href="tree-based-models.html#cb46-6" aria-hidden="true" tabindex="-1"></a><span class="co"># evaluate random forest (model diagnostics)</span></span>
<span id="cb46-7"><a href="tree-based-models.html#cb46-7" aria-hidden="true" tabindex="-1"></a>rfmodel1_pred <span class="ot">&lt;-</span> <span class="fu">unlist</span>(party<span class="sc">::</span><span class="fu">treeresponse</span>(rfmodel1))<span class="co">#[c(FALSE,TRUE)]</span></span>
<span id="cb46-8"><a href="tree-based-models.html#cb46-8" aria-hidden="true" tabindex="-1"></a><span class="fu">somers2</span>(rfmodel1_pred, <span class="fu">as.numeric</span>(rfdata<span class="sc">$</span>SUFlike))</span></code></pre></div>
<pre><code>##            C          Dxy            n      Missing 
##    0.7112131    0.4224262 2000.0000000    0.0000000</code></pre>
<p>The model parameters are excellent: remember that if the C-value is 0.5, the predictions are random, while the predictions are perfect if the C-value is 1. C-values above 0.8 indicate real predictive capacity <span class="citation">(<a href="#ref-baayen2008analyzing" role="doc-biblioref">Baayen 2008, 204</a>)</span>. Somers’ D<sub>xy</sub> is a value that represents a rank correlation between predicted probabilities and observed responses. Somers’ D<sub>xy</sub> values range between 0, which indicates complete randomness, and 1, which indicates perfect prediction <span class="citation">(<a href="#ref-baayen2008analyzing" role="doc-biblioref">Baayen 2008, 204</a>)</span>.</p>
<p>In a next step, we extract the variable importance <code>conditional=T</code> adjusts for correlations between predictors).</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb48-1"><a href="tree-based-models.html#cb48-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract variable importance based on mean decrease in accuracy</span></span>
<span id="cb48-2"><a href="tree-based-models.html#cb48-2" aria-hidden="true" tabindex="-1"></a>rfmodel1_varimp <span class="ot">&lt;-</span> <span class="fu">varimp</span>(rfmodel1, <span class="at">conditional =</span> T) </span>
<span id="cb48-3"><a href="tree-based-models.html#cb48-3" aria-hidden="true" tabindex="-1"></a><span class="co"># show variable importance</span></span>
<span id="cb48-4"><a href="tree-based-models.html#cb48-4" aria-hidden="true" tabindex="-1"></a>rfmodel1_varimp</span></code></pre></div>
<pre><code>##           Gender              Age ConversationType          Priming 
##      0.003770260      0.000542920      0.002520164      0.022095496</code></pre>
<p>We can also calculate more robust variable importance using the <code>varimpAUC</code> function from the <code>party</code> package which calculates importance statistics that are corrected towards class imbalance, i.e. differences in the number of instances per category. The variable importance is easily visualized using the <code>dotplot</code> function from base R.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb50-1"><a href="tree-based-models.html#cb50-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract more robust variable importance </span></span>
<span id="cb50-2"><a href="tree-based-models.html#cb50-2" aria-hidden="true" tabindex="-1"></a>rfmodel1_robustvarimp <span class="ot">&lt;-</span> party<span class="sc">::</span><span class="fu">varimp</span>(rfmodel1)  </span>
<span id="cb50-3"><a href="tree-based-models.html#cb50-3" aria-hidden="true" tabindex="-1"></a><span class="co"># plot result</span></span>
<span id="cb50-4"><a href="tree-based-models.html#cb50-4" aria-hidden="true" tabindex="-1"></a><span class="fu">dotchart</span>(<span class="fu">sort</span>(rfmodel1_robustvarimp), <span class="at">pch =</span> <span class="dv">20</span>, <span class="at">main =</span> <span class="st">&quot;Conditional importance of variables&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/rf8-1.png" width="672" /></p>
<p>The plot shows that Age is the most important predictor and that Priming is not really important as a predictor for speech-unit final like. Gender and ConversationType are equally important but both much less so than Age.</p>
<p>We will now use an alternative way to calculate RFs which allows us to use different diagnostics and pruning techniques by using the <code>randomForest</code> rather than the <code>cforest</code> function.</p>
<p>A few words on the parameters of the <code>randomForest</code> function: if the thing we’re trying to predict is a numeric variable, the <code>randomForest</code> function will set <code>mtry</code> (the number of variables considered at each step) to the total number of variables divided by 3 (rounded down), or to 1 if the division results in a value less than 1. If the thing we’re trying to predict is a “factor” (i.e. either “yes/no” or “ranked”), then <code>randomForest()</code> will set <code>mtry</code> to the square root of the number of variables (rounded down to the next integer value).Again, we start by setting a seed to store random numbers and thus make results reproducible.</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb51-1"><a href="tree-based-models.html#cb51-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed</span></span>
<span id="cb51-2"><a href="tree-based-models.html#cb51-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019120205</span>)</span>
<span id="cb51-3"><a href="tree-based-models.html#cb51-3" aria-hidden="true" tabindex="-1"></a>rfmodel2 <span class="ot">&lt;-</span> randomForest<span class="sc">::</span><span class="fu">randomForest</span>(SUFlike <span class="sc">~</span> ., </span>
<span id="cb51-4"><a href="tree-based-models.html#cb51-4" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">data=</span>rfdata, </span>
<span id="cb51-5"><a href="tree-based-models.html#cb51-5" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">mtry =</span> <span class="dv">2</span>,</span>
<span id="cb51-6"><a href="tree-based-models.html#cb51-6" aria-hidden="true" tabindex="-1"></a>                                       <span class="at">proximity=</span><span class="cn">TRUE</span>)</span>
<span id="cb51-7"><a href="tree-based-models.html#cb51-7" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect model</span></span>
<span id="cb51-8"><a href="tree-based-models.html#cb51-8" aria-hidden="true" tabindex="-1"></a>rfmodel2 </span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = SUFlike ~ ., data = rfdata, mtry = 2,      proximity = TRUE) 
##                Type of random forest: regression
##                      Number of trees: 500
## No. of variables tried at each split: 2
## 
##           Mean of squared residuals: 0.1277716
##                     % Var explained: 10.28</code></pre>
<p>The output tells us that the model explains less than 15 percent of the variance. It is recommendable to check if changing parameters causes and increase in the amount of variance that is explained by a model (which is desirable). In this case, we can try different values for <code>mtry</code> and for <code>ntree</code> as shown below and then compare the performance of the random forest models by inspecting the amount of variance that they explain. Again, we begin by setting a seed and then continue by specifying the random forest model.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb53-1"><a href="tree-based-models.html#cb53-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed (to store random numbers and thus make results reproducible)</span></span>
<span id="cb53-2"><a href="tree-based-models.html#cb53-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019120206</span>)</span>
<span id="cb53-3"><a href="tree-based-models.html#cb53-3" aria-hidden="true" tabindex="-1"></a><span class="co"># create a new model with fewer trees and that takes 2 variables at a time</span></span>
<span id="cb53-4"><a href="tree-based-models.html#cb53-4" aria-hidden="true" tabindex="-1"></a>rfmodel3 <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(SUFlike <span class="sc">~</span> ., <span class="at">data=</span>rfdata, <span class="at">ntree=</span><span class="dv">30</span>, <span class="at">mtry =</span> <span class="dv">4</span>)</span>
<span id="cb53-5"><a href="tree-based-models.html#cb53-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect model</span></span>
<span id="cb53-6"><a href="tree-based-models.html#cb53-6" aria-hidden="true" tabindex="-1"></a>rfmodel3</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = SUFlike ~ ., data = rfdata, ntree = 30,      mtry = 4) 
##                Type of random forest: regression
##                      Number of trees: 30
## No. of variables tried at each split: 4
## 
##           Mean of squared residuals: 0.1283899
##                     % Var explained: 9.85</code></pre>
<p>Despite optimization, the results have not changed but it may be very useful for other data. To evaluate the tree, we create a confusion matrix.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb55-1"><a href="tree-based-models.html#cb55-1" aria-hidden="true" tabindex="-1"></a><span class="co"># save what the model predicted in a new variable</span></span>
<span id="cb55-2"><a href="tree-based-models.html#cb55-2" aria-hidden="true" tabindex="-1"></a>rfdata<span class="sc">$</span>Probability <span class="ot">&lt;-</span> <span class="fu">predict</span>(rfmodel3, rfdata)</span>
<span id="cb55-3"><a href="tree-based-models.html#cb55-3" aria-hidden="true" tabindex="-1"></a>rfdata<span class="sc">$</span>Prediction <span class="ot">&lt;-</span> <span class="fu">ifelse</span>(rfdata<span class="sc">$</span>Probability <span class="sc">&gt;=</span>.<span class="dv">5</span>, <span class="dv">1</span>, <span class="dv">0</span>)</span>
<span id="cb55-4"><a href="tree-based-models.html#cb55-4" aria-hidden="true" tabindex="-1"></a><span class="co"># create confusion matrix to check accuracy</span></span>
<span id="cb55-5"><a href="tree-based-models.html#cb55-5" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">as.factor</span>(rfdata<span class="sc">$</span>Prediction), <span class="fu">as.factor</span>(rfdata<span class="sc">$</span>SUFlike))  </span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction    0    1
##          0 1622  297
##          1   34   47
##                                           
##                Accuracy : 0.8345          
##                  95% CI : (0.8175, 0.8505)
##     No Information Rate : 0.828           
##     P-Value [Acc &gt; NIR] : 0.2303          
##                                           
##                   Kappa : 0.1665          
##                                           
##  Mcnemar&#39;s Test P-Value : &lt;2e-16          
##                                           
##             Sensitivity : 0.9795          
##             Specificity : 0.1366          
##          Pos Pred Value : 0.8452          
##          Neg Pred Value : 0.5802          
##              Prevalence : 0.8280          
##          Detection Rate : 0.8110          
##    Detection Prevalence : 0.9595          
##       Balanced Accuracy : 0.5580          
##                                           
##        &#39;Positive&#39; Class : 0               
## </code></pre>
<p>The RF performs significantly better than a no-information base-line model but the base-line model already predicts 78.18 percent of cases correctly (compared to the RF with a prediction accuracy of 82.5 percent).</p>
<p>Unfortunately, we cannot easily compute robust variable importance for RF models nor C or Somers’ D<sub>xy</sub> which is why it is advisable to create analogous models using both the <code>cforest</code> and the <code>randomForest</code> functions. In a last step, we can now visualize the results of the optimized RF.</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb57-1"><a href="tree-based-models.html#cb57-1" aria-hidden="true" tabindex="-1"></a><span class="co"># plot variable importance</span></span>
<span id="cb57-2"><a href="tree-based-models.html#cb57-2" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rfmodel3, <span class="at">main =</span> <span class="st">&quot;&quot;</span>, <span class="at">pch =</span> <span class="dv">20</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/rf15-1.png" width="672" /></p>
<p>A second type of visualization that can provide insights in the <code>partial</code> function from the <code>pdp</code> package to show how the effect of predictors interacts with other predictors (see below).</p>
<div class="sourceCode" id="cb58"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb58-1"><a href="tree-based-models.html#cb58-1" aria-hidden="true" tabindex="-1"></a><span class="fu">partial</span>(rfmodel3, <span class="at">pred.var =</span> <span class="fu">c</span>(<span class="st">&quot;Age&quot;</span>, <span class="st">&quot;Gender&quot;</span>), <span class="at">plot =</span> <span class="cn">TRUE</span>, <span class="at">plot.engine =</span> <span class="st">&quot;ggplot2&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/rf19-1.png" width="672" /></p>
<p>Another common way to evaluate the performance of RFs is to split the data into a test and a training set. the model is then fit to the training set and, after that, applied to the test set. This allows us to evaluate how well the RF performs on data that it was not trained on. This approach is particularly common in machine learning contexts.</p>
</div>
<div id="boruta" class="section level2 hasAnchor" number="3.7">
<h2><span class="header-section-number">3.7</span> Boruta<a href="tree-based-models.html#boruta" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Boruta <span class="citation">(<a href="#ref-kursa2010feature" role="doc-biblioref">Kursa, Rudnicki, et al. 2010</a>)</span> is a variable selection procedure and it represents an extension of random forest analyses <span class="citation">(<a href="#ref-breiman2001random" role="doc-biblioref">Breiman 2001a</a>)</span>. The name <em>Boruta</em> is derived from a demon in Slavic mythology who dwelled in pine forests. Boruta is an alternative to regression modeling that is better equipped to handle small data sets because it uses a distributional approach during which hundreds of (random) forests are grown from permuted data sets.</p>
<div id="advantages-1" class="section level3 unnumbered hasAnchor">
<h3>Advantages<a href="tree-based-models.html#advantages-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Boruta outperforms random forest analyses because:</p>
<ul>
<li><p>Boruta does not provide merely a single value for each predictor but a distribution of values leading to higher reliability.</p></li>
<li><p>Boruta provides definitive cut-off points for variables that have no meaningful relationship with the dependent variable. This is a crucial difference between RF and Boruta that make Boruta particularly interesting from a variable selection point of view.</p></li>
</ul>
</div>
<div id="procedure" class="section level3 unnumbered hasAnchor">
<h3>Procedure<a href="tree-based-models.html#procedure" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>The Boruta procedure consists out of five steps.</p>
<ul>
<li><p>In a first step, the Boruta algorithm copies the data set and adds randomness to the data by (re-)shuffling data points and thereby creating randomized variables. These randomized variables are referred to as shadow features.</p></li>
<li><p>Secondly, a random forest classifier is trained on the extended data set.</p></li>
<li><p>In a third step, a feature importance measure (Mean Decrease Accuracy represented by z-scores) is calculated to determine the relative importance of all predictors (both original or real variables and the randomized shadow features).</p></li>
<li><p>In the next step, it is checked at each iteration of the process whether a real predictor has a higher importance compared with the best shadow feature. The algorithm keeps track of the performance of the original variables by storing whether they outperformed the best shadow feature or not in a vector.</p></li>
<li><p>In the fifth step, predictors that did not outperform the best shadow feature are removed and the process continues without them. After a set number of iterations, or if all the variables have been either confirmed as outperforming the best shadow feature, the algorithm stops.</p></li>
</ul>
<p>Despite its obvious advantages of Boruta over random forest analyses and regression modeling, it can neither handle multicollinearity not hierarchical data structures where data points are nested or grouped by a given predictor (as is the case in the present analysis as data points are grouped by adjective type). As Boruta is a variable selection procedure, it is also limited in the sense that it provides information on which predictors to include and how good these predictors are (compared to the shadow variables) while it is neither able to take hierarchical data structure into account, nor does it provide information about how one level of a factor compares to other factors. In other words, Boruta shows that a predictor is relevant and how strong it is but it does not provide information on how the likelihood of an outcome being used differs between variable levels, for instance between men and women.</p>
</div>
<div id="boruta-in-r" class="section level3 hasAnchor" number="3.7.1">
<h3><span class="header-section-number">3.7.1</span> Boruta in R<a href="tree-based-models.html#boruta-in-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>We begin by loading and inspecting the data.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb59-1"><a href="tree-based-models.html#cb59-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load data</span></span>
<span id="cb59-2"><a href="tree-based-models.html#cb59-2" aria-hidden="true" tabindex="-1"></a>borutadata <span class="ot">&lt;-</span> <span class="fu">read.delim</span>(<span class="st">&quot;https://slcladal.github.io/data/ampaus05_statz.txt&quot;</span>, <span class="at">header =</span> T, <span class="at">sep =</span> <span class="st">&quot;</span><span class="sc">\t</span><span class="st">&quot;</span>)</span>
<span id="cb59-3"><a href="tree-based-models.html#cb59-3" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect data</span></span>
<span id="cb59-4"><a href="tree-based-models.html#cb59-4" aria-hidden="true" tabindex="-1"></a>borutadata <span class="sc">%&gt;%</span></span>
<span id="cb59-5"><a href="tree-based-models.html#cb59-5" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb59-6"><a href="tree-based-models.html#cb59-6" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">10</span>)</span></code></pre></div>
<pre><code>##      Age Adjective   FileSpeaker    Function Priming Gender
## 1  26-40      good &lt;S1A-001:1$B&gt; Attributive NoPrime    Men
## 2  26-40      good &lt;S1A-001:1$B&gt; Attributive NoPrime    Men
## 3  26-40      good &lt;S1A-001:1$B&gt; Predicative NoPrime    Men
## 4  17-25      nice &lt;S1A-003:1$B&gt; Attributive NoPrime    Men
## 5  41-80     other &lt;S1A-003:1$A&gt; Predicative NoPrime    Men
## 6  41-80     other &lt;S1A-004:1$C&gt; Predicative NoPrime    Men
## 7  41-80      good &lt;S1A-004:1$B&gt; Attributive NoPrime  Women
## 8  41-80     other &lt;S1A-005:1$B&gt; Predicative NoPrime  Women
## 9  17-25     other &lt;S1A-006:1$B&gt; Attributive NoPrime    Men
## 10 17-25     other &lt;S1A-006:1$B&gt; Attributive   Prime    Men
##                         Occupation ConversationType          AudienceSize very
## 1  AcademicManagerialProfessionals          SameSex MultipleInterlocutors    0
## 2  AcademicManagerialProfessionals          SameSex MultipleInterlocutors    0
## 3  AcademicManagerialProfessionals          SameSex MultipleInterlocutors    0
## 4  AcademicManagerialProfessionals          SameSex                  Dyad    1
## 5  AcademicManagerialProfessionals          SameSex                  Dyad    0
## 6                             &lt;NA&gt;         MixedSex MultipleInterlocutors    1
## 7  AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
## 8                             &lt;NA&gt;         MixedSex MultipleInterlocutors    1
## 9  AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
## 10 AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
##    really     Freq              Gradabilty SemanticCategory      Emotionality
## 1       0 27.84810             NotGradable            Value PositiveEmotional
## 2       0 27.84810             NotGradable            Value PositiveEmotional
## 3       0 27.84810             NotGradable            Value PositiveEmotional
## 4       0  7.29282             NotGradable  HumanPropensity      NonEmotional
## 5       0  0.61728             NotGradable            Value      NonEmotional
## 6       0  2.46914             NotGradable            Value PositiveEmotional
## 7       0 20.98765             NotGradable            Value PositiveEmotional
## 8       0  0.61728 GradabilityUndetermined  HumanPropensity NegativeEmotional
## 9       1  4.64088 GradabilityUndetermined        Dimension      NonEmotional
## 10      1  0.44199             NotGradable PhysicalProperty      NonEmotional</code></pre>
<p>As the data contains non-factorized character variables, we convert those into factors.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb61-1"><a href="tree-based-models.html#cb61-1" aria-hidden="true" tabindex="-1"></a><span class="co"># factorize variables (boruta - like rf - require factors instead of character vectors)</span></span>
<span id="cb61-2"><a href="tree-based-models.html#cb61-2" aria-hidden="true" tabindex="-1"></a>borutadata <span class="ot">&lt;-</span> borutadata <span class="sc">%&gt;%</span></span>
<span id="cb61-3"><a href="tree-based-models.html#cb61-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">filter</span>(<span class="fu">complete.cases</span>(.)) <span class="sc">%&gt;%</span></span>
<span id="cb61-4"><a href="tree-based-models.html#cb61-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate_if</span>(is.character, factor)</span>
<span id="cb61-5"><a href="tree-based-models.html#cb61-5" aria-hidden="true" tabindex="-1"></a><span class="co"># inspect data</span></span>
<span id="cb61-6"><a href="tree-based-models.html#cb61-6" aria-hidden="true" tabindex="-1"></a>borutadata <span class="sc">%&gt;%</span></span>
<span id="cb61-7"><a href="tree-based-models.html#cb61-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">as.data.frame</span>() <span class="sc">%&gt;%</span></span>
<span id="cb61-8"><a href="tree-based-models.html#cb61-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">head</span>(<span class="dv">10</span>) </span></code></pre></div>
<pre><code>##      Age Adjective   FileSpeaker    Function Priming Gender
## 1  26-40      good &lt;S1A-001:1$B&gt; Attributive NoPrime    Men
## 2  26-40      good &lt;S1A-001:1$B&gt; Attributive NoPrime    Men
## 3  26-40      good &lt;S1A-001:1$B&gt; Predicative NoPrime    Men
## 4  17-25      nice &lt;S1A-003:1$B&gt; Attributive NoPrime    Men
## 5  41-80     other &lt;S1A-003:1$A&gt; Predicative NoPrime    Men
## 6  41-80      good &lt;S1A-004:1$B&gt; Attributive NoPrime  Women
## 7  17-25     other &lt;S1A-006:1$B&gt; Attributive NoPrime    Men
## 8  17-25     other &lt;S1A-006:1$B&gt; Attributive   Prime    Men
## 9  17-25     other &lt;S1A-006:1$B&gt; Predicative   Prime    Men
## 10 17-25      nice &lt;S1A-007:1$A&gt; Attributive NoPrime  Women
##                         Occupation ConversationType          AudienceSize very
## 1  AcademicManagerialProfessionals          SameSex MultipleInterlocutors    0
## 2  AcademicManagerialProfessionals          SameSex MultipleInterlocutors    0
## 3  AcademicManagerialProfessionals          SameSex MultipleInterlocutors    0
## 4  AcademicManagerialProfessionals          SameSex                  Dyad    1
## 5  AcademicManagerialProfessionals          SameSex                  Dyad    0
## 6  AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
## 7  AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
## 8  AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
## 9  AcademicManagerialProfessionals         MixedSex MultipleInterlocutors    0
## 10 AcademicManagerialProfessionals          SameSex                  Dyad    0
##    really     Freq              Gradabilty SemanticCategory      Emotionality
## 1       0 27.84810             NotGradable            Value PositiveEmotional
## 2       0 27.84810             NotGradable            Value PositiveEmotional
## 3       0 27.84810             NotGradable            Value PositiveEmotional
## 4       0  7.29282             NotGradable  HumanPropensity      NonEmotional
## 5       0  0.61728             NotGradable            Value      NonEmotional
## 6       0 20.98765             NotGradable            Value PositiveEmotional
## 7       1  4.64088 GradabilityUndetermined        Dimension      NonEmotional
## 8       1  0.44199             NotGradable PhysicalProperty      NonEmotional
## 9       1  0.44199             NotGradable PhysicalProperty      NonEmotional
## 10      1  7.29282             NotGradable  HumanPropensity      NonEmotional</code></pre>
<p>We can now create our initial Boruta model and set a seed for reproducibility.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb63-1"><a href="tree-based-models.html#cb63-1" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed </span></span>
<span id="cb63-2"><a href="tree-based-models.html#cb63-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019120207</span>)</span>
<span id="cb63-3"><a href="tree-based-models.html#cb63-3" aria-hidden="true" tabindex="-1"></a><span class="co"># initial run</span></span>
<span id="cb63-4"><a href="tree-based-models.html#cb63-4" aria-hidden="true" tabindex="-1"></a>boruta1 <span class="ot">&lt;-</span> <span class="fu">Boruta</span>(really<span class="sc">~</span>.,<span class="at">data=</span>borutadata)</span>
<span id="cb63-5"><a href="tree-based-models.html#cb63-5" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(boruta1)</span></code></pre></div>
<pre><code>## Boruta performed 99 iterations in 7.389485 secs.
##  8 attributes confirmed important: Adjective, AudienceSize,
## ConversationType, Emotionality, FileSpeaker and 3 more;
##  5 attributes confirmed unimportant: Age, Gender, Gradabilty,
## Occupation, Priming;
##  1 tentative attributes left: SemanticCategory;</code></pre>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb65-1"><a href="tree-based-models.html#cb65-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract decision</span></span>
<span id="cb65-2"><a href="tree-based-models.html#cb65-2" aria-hidden="true" tabindex="-1"></a><span class="fu">getConfirmedFormula</span>(boruta1)</span></code></pre></div>
<pre><code>## really ~ Adjective + FileSpeaker + Function + ConversationType + 
##     AudienceSize + very + Freq + Emotionality
## &lt;environment: 0x0000023d48c6a240&gt;</code></pre>
<p>In a next step, we inspect the history to check if any of the variables shows drastic fluctuations in their importance assessment.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb67-1"><a href="tree-based-models.html#cb67-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plotImpHistory</span>(boruta1)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/bo5-1.png" width="672" /></p>
<p>The fluctuations are do not show clear upward or downward trends (which what we want). If predictors do perform worse than the shadow variables, then these variables should be excluded and the Boruta analysis should be re-run on the data set that does no longer contain the superfluous variables. Tentative variables can remain but they are unlikely to have any substantial effect. We thus continue by removing variables that were confirmed as being unimportant, then setting a new seed, re-running the Boruta on the reduced data set, and again inspecting the decisions.</p>
<div class="sourceCode" id="cb68"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb68-1"><a href="tree-based-models.html#cb68-1" aria-hidden="true" tabindex="-1"></a><span class="co"># remove irrelevant variables</span></span>
<span id="cb68-2"><a href="tree-based-models.html#cb68-2" aria-hidden="true" tabindex="-1"></a>rejected <span class="ot">&lt;-</span> <span class="fu">names</span>(boruta1<span class="sc">$</span>finalDecision)[<span class="fu">which</span>(boruta1<span class="sc">$</span>finalDecision <span class="sc">==</span> <span class="st">&quot;Rejected&quot;</span>)]</span>
<span id="cb68-3"><a href="tree-based-models.html#cb68-3" aria-hidden="true" tabindex="-1"></a><span class="co"># update data for boruta</span></span>
<span id="cb68-4"><a href="tree-based-models.html#cb68-4" aria-hidden="true" tabindex="-1"></a>borutadata <span class="ot">&lt;-</span> borutadata <span class="sc">%&gt;%</span></span>
<span id="cb68-5"><a href="tree-based-models.html#cb68-5" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">select</span>(<span class="sc">-</span>rejected)</span>
<span id="cb68-6"><a href="tree-based-models.html#cb68-6" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed (to store random numbers and thus make results reproducible)</span></span>
<span id="cb68-7"><a href="tree-based-models.html#cb68-7" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">2019120208</span>)</span>
<span id="cb68-8"><a href="tree-based-models.html#cb68-8" aria-hidden="true" tabindex="-1"></a><span class="co"># 2nd run</span></span>
<span id="cb68-9"><a href="tree-based-models.html#cb68-9" aria-hidden="true" tabindex="-1"></a>boruta2 <span class="ot">&lt;-</span> <span class="fu">Boruta</span>(really<span class="sc">~</span>.,<span class="at">data=</span>borutadata)</span>
<span id="cb68-10"><a href="tree-based-models.html#cb68-10" aria-hidden="true" tabindex="-1"></a><span class="fu">print</span>(boruta2)</span></code></pre></div>
<pre><code>## Boruta performed 99 iterations in 7.06097 secs.
##  8 attributes confirmed important: Adjective, AudienceSize,
## ConversationType, Emotionality, FileSpeaker and 3 more;
##  No attributes deemed unimportant.
##  1 tentative attributes left: SemanticCategory;</code></pre>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb70-1"><a href="tree-based-models.html#cb70-1" aria-hidden="true" tabindex="-1"></a><span class="co"># extract decision</span></span>
<span id="cb70-2"><a href="tree-based-models.html#cb70-2" aria-hidden="true" tabindex="-1"></a><span class="fu">getConfirmedFormula</span>(boruta2)</span></code></pre></div>
<pre><code>## really ~ Adjective + FileSpeaker + Function + ConversationType + 
##     AudienceSize + very + Freq + Emotionality
## &lt;environment: 0x0000023d4be2f770&gt;</code></pre>
<p>Only adjective frequency and adjective type are confirmed as being important while all other variables are considered tentative. However, no more variables need to be removed as all remaining variables are not considered unimportant. In a last step, we visualize the results of the Boruta analysis.</p>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb72-1"><a href="tree-based-models.html#cb72-1" aria-hidden="true" tabindex="-1"></a>borutadf <span class="ot">&lt;-</span> <span class="fu">as.data.frame</span>(boruta2<span class="sc">$</span>ImpHistory) <span class="sc">%&gt;%</span></span>
<span id="cb72-2"><a href="tree-based-models.html#cb72-2" aria-hidden="true" tabindex="-1"></a>  tidyr<span class="sc">::</span><span class="fu">gather</span>(Variable, Importance, Adjective<span class="sc">:</span>shadowMin) <span class="sc">%&gt;%</span></span>
<span id="cb72-3"><a href="tree-based-models.html#cb72-3" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">Type =</span> <span class="fu">ifelse</span>(<span class="fu">str_detect</span>(Variable, <span class="st">&quot;shadow&quot;</span>), <span class="st">&quot;Control&quot;</span>, <span class="st">&quot;Predictor&quot;</span>)) <span class="sc">%&gt;%</span></span>
<span id="cb72-4"><a href="tree-based-models.html#cb72-4" aria-hidden="true" tabindex="-1"></a>  dplyr<span class="sc">::</span><span class="fu">mutate</span>(<span class="at">Type =</span> <span class="fu">factor</span>(Type),</span>
<span id="cb72-5"><a href="tree-based-models.html#cb72-5" aria-hidden="true" tabindex="-1"></a>                <span class="at">Variable =</span> <span class="fu">factor</span>(Variable))</span>
<span id="cb72-6"><a href="tree-based-models.html#cb72-6" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(borutadf, <span class="fu">aes</span>(<span class="at">x =</span> <span class="fu">reorder</span>(Variable, Importance, mean), <span class="at">y =</span> Importance, <span class="at">fill =</span> Type)) <span class="sc">+</span> </span>
<span id="cb72-7"><a href="tree-based-models.html#cb72-7" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_boxplot</span>() <span class="sc">+</span></span>
<span id="cb72-8"><a href="tree-based-models.html#cb72-8" aria-hidden="true" tabindex="-1"></a>  <span class="fu">geom_vline</span>(<span class="at">xintercept=</span><span class="fl">3.5</span>, <span class="at">linetype=</span><span class="st">&quot;dashed&quot;</span>, <span class="at">color =</span> <span class="st">&quot;black&quot;</span>) <span class="sc">+</span></span>
<span id="cb72-9"><a href="tree-based-models.html#cb72-9" aria-hidden="true" tabindex="-1"></a>  <span class="fu">scale_fill_manual</span>(<span class="at">values =</span> <span class="fu">c</span>(<span class="st">&quot;gray80&quot;</span>, <span class="st">&quot;gray40&quot;</span>)) <span class="sc">+</span></span>
<span id="cb72-10"><a href="tree-based-models.html#cb72-10" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme_bw</span>() <span class="sc">+</span> </span>
<span id="cb72-11"><a href="tree-based-models.html#cb72-11" aria-hidden="true" tabindex="-1"></a>  <span class="fu">theme</span>(<span class="at">legend.position =</span> <span class="st">&quot;top&quot;</span>,</span>
<span id="cb72-12"><a href="tree-based-models.html#cb72-12" aria-hidden="true" tabindex="-1"></a>        <span class="at">axis.text.x =</span> <span class="fu">element_text</span>(<span class="at">angle=</span><span class="dv">90</span>)) <span class="sc">+</span></span>
<span id="cb72-13"><a href="tree-based-models.html#cb72-13" aria-hidden="true" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Variable&quot;</span>)</span></code></pre></div>
<p><img src="bookdownproj_files/figure-html/bo7-1.png" width="672" /></p>
<p>Of the remaining variables, adjective frequency and adjective type have the strongest effect and are confirmed as being important while syntactic function fails to perform better than the best shadow variable. All other variables have only a marginal effect on the use of really as an adjective amplifier.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb73-1"><a href="tree-based-models.html#cb73-1" aria-hidden="true" tabindex="-1"></a><span class="fu">sessionInfo</span>()</span></code></pre></div>
<pre><code>## R version 4.2.0 (2022-04-22 ucrt)
## Platform: x86_64-w64-mingw32/x64 (64-bit)
## Running under: Windows 10 x64 (build 19043)
## 
## Matrix products: default
## 
## locale:
## [1] LC_COLLATE=English_Australia.utf8  LC_CTYPE=English_Australia.utf8   
## [3] LC_MONETARY=English_Australia.utf8 LC_NUMERIC=C                      
## [5] LC_TIME=English_Australia.utf8    
## 
## attached base packages:
## [1] stats4    grid      stats     graphics  grDevices utils     datasets 
## [8] methods   base     
## 
## other attached packages:
##  [1] vip_0.3.2            RCurl_1.98-1.7       pdp_0.8.1           
##  [4] randomForest_4.7-1.1 party_1.3-10         strucchange_1.5-2   
##  [7] sandwich_3.0-1       zoo_1.8-10           modeltools_0.2-23   
## [10] Hmisc_4.7-0          Formula_1.2-4        survival_3.3-1      
## [13] Gmisc_3.0.0          htmlTable_2.4.0      Rcpp_1.0.8.3        
## [16] ggparty_1.0.0        partykit_1.2-15      mvtnorm_1.1-3       
## [19] libcoin_1.0-9        forcats_0.5.1        stringr_1.4.0       
## [22] dplyr_1.0.9          purrr_0.3.4          readr_2.1.2         
## [25] tidyr_1.2.0          tibble_3.1.7         tidyverse_1.3.1     
## [28] cowplot_1.1.1        caret_6.0-92         lattice_0.20-45     
## [31] ggplot2_3.3.6        tree_1.0-41          Boruta_7.0.0        
## 
## loaded via a namespace (and not attached):
##   [1] readxl_1.4.0         backports_1.4.1      plyr_1.8.7          
##   [4] splines_4.2.0        listenv_0.8.0        TH.data_1.1-1       
##   [7] digest_0.6.29        foreach_1.5.2        htmltools_0.5.2     
##  [10] fansi_1.0.3          magrittr_2.0.3       checkmate_2.1.0     
##  [13] cluster_2.1.3        tzdb_0.3.0           recipes_0.2.0       
##  [16] globals_0.15.0       modelr_0.1.8         gower_1.0.0         
##  [19] matrixStats_0.62.0   hardhat_1.0.0        jpeg_0.1-9          
##  [22] colorspace_2.0-3     rvest_1.0.2          haven_2.5.0         
##  [25] xfun_0.31            crayon_1.5.1         jsonlite_1.8.0      
##  [28] iterators_1.0.14     glue_1.6.2           gtable_0.3.0        
##  [31] ipred_0.9-12         future.apply_1.9.0   abind_1.4-5         
##  [34] scales_1.2.0         DBI_1.1.2            proxy_0.4-26        
##  [37] foreign_0.8-82       lava_1.6.10          prodlim_2019.11.13  
##  [40] htmlwidgets_1.5.4    httr_1.4.3           RColorBrewer_1.1-3  
##  [43] ellipsis_0.3.2       farver_2.1.0         pkgconfig_2.0.3     
##  [46] XML_3.99-0.9         nnet_7.3-17          sass_0.4.1          
##  [49] dbplyr_2.1.1         here_1.0.1           utf8_1.2.2          
##  [52] labeling_0.4.2       tidyselect_1.1.2     rlang_1.0.2         
##  [55] reshape2_1.4.4       munsell_0.5.0        cellranger_1.1.0    
##  [58] tools_4.2.0          cli_3.3.0            generics_0.1.2      
##  [61] ranger_0.13.1        broom_0.8.0          evaluate_0.15       
##  [64] fastmap_1.1.0        yaml_2.3.5           ModelMetrics_1.2.2.2
##  [67] knitr_1.39           fs_1.5.2             forestplot_2.0.1    
##  [70] coin_1.4-2           future_1.26.1        nlme_3.1-157        
##  [73] xml2_1.3.3           compiler_4.2.0       rstudioapi_0.13     
##  [76] png_0.1-7            e1071_1.7-9          reprex_2.0.1        
##  [79] bslib_0.3.1          stringi_1.7.6        highr_0.9           
##  [82] Matrix_1.4-1         vctrs_0.4.1          pillar_1.7.0        
##  [85] lifecycle_1.0.1      jquerylib_0.1.4      data.table_1.14.2   
##  [88] bitops_1.0-7         R6_2.5.1             latticeExtra_0.6-29 
##  [91] bookdown_0.27        gridExtra_2.3        parallelly_1.31.1   
##  [94] codetools_0.2-18     MASS_7.3-57          assertthat_0.2.1    
##  [97] rprojroot_2.0.3      withr_2.5.0          multcomp_1.4-19     
## [100] parallel_4.2.0       hms_1.1.1            rpart_4.1.16        
## [103] timeDate_3043.102    class_7.3-20         rmarkdown_2.14      
## [106] inum_1.0-4           pROC_1.18.0          lubridate_1.8.0     
## [109] base64enc_0.1-3</code></pre>
</div>
</div>
<div id="references" class="section level2 hasAnchor" number="3.8">
<h2><span class="header-section-number">3.8</span> References<a href="tree-based-models.html#references" class="anchor-section" aria-label="Anchor link to header"></a></h2>

<div id="refs" class="references csl-bib-body hanging-indent">
<div class="csl-entry">
Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. A Practical Introduction to Statistics Using r</em>. Cambridge: Cambridge University press.
</div>
<div class="csl-entry">
Boulesteix, Anne-Laure, Silke Janitza, Alexander Hapfelmeier, Kristel Van Steen, and Carolin Strobl. 2015. <span>“Letter to the Editor: On the Term <span>‘Interaction’</span> and Related Phrases in the Literature on RandomForests.”</span> <em>Briefings in Bioinformatics</em> 16 (2): 338–45. <a href="https://academic.oup.com/bib/article/16/2/338/246566">https://academic.oup.com/bib/article/16/2/338/246566</a>.
</div>
<div class="csl-entry">
Breiman, Leo. 2001a. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div class="csl-entry">
———. 2001b. <span>“Statistical Modeling: The Two Cultures.”</span> <em>Statistical Science</em> 16: 199–231. <a href="https://projecteuclid.org/euclid.ss/1009213726">https://projecteuclid.org/euclid.ss/1009213726</a>.
</div>
<div class="csl-entry">
Gries, Stefan Th. 2021. <em>Statistics for Linguistics Using r: A Practical Introduction</em>. Berlin &amp; New York: Mouton de Gruyter.
</div>
<div class="csl-entry">
Kursa, Miron B, Witold R Rudnicki, et al. 2010. <span>“Feature Selection with the Boruta Package.”</span> <em>Journal of Statistical Software</em> 36 (11): 1–13.
</div>
<div class="csl-entry">
Prasad, Anantha M, Louis R Iverson, and Andy Liaw. 2006. <span>“Newer Classification and Regression Tree Techniques: Bagging and Random Forests for Ecological Prediction.”</span> <em>Ecosystems</em> 9 (2): 181–99.
</div>
<div class="csl-entry">
Strobl, Carolin, James Malley, and Gerhard Tutz. 2009. <span>“An Introduction to Recursive Partitioning: Rationale,application and Characteristics of Classification and Regression Trees, Bagging and Random Forests.”</span> <em>Psychological Methods</em> 14 (4): 323–48. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/</a>.
</div>
<div class="csl-entry">
Wright, Marvin N., Andreas Ziegler, and Inke R. König. 2016. <span>“Do Little Interactions Get Lost in Dark Random Forests?”</span> 17 (145). <a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-0995-8">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-0995-8</a>.
</div>
</div>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-baayen2008analyzing" class="csl-entry">
Baayen, R Harald. 2008. <em>Analyzing Linguistic Data. A Practical Introduction to Statistics Using r</em>. Cambridge: Cambridge University press.
</div>
<div id="ref-boulesteix2015interaction" class="csl-entry">
Boulesteix, Anne-Laure, Silke Janitza, Alexander Hapfelmeier, Kristel Van Steen, and Carolin Strobl. 2015. <span>“Letter to the Editor: On the Term <span>‘Interaction’</span> and Related Phrases in the Literature on RandomForests.”</span> <em>Briefings in Bioinformatics</em> 16 (2): 338–45. <a href="https://academic.oup.com/bib/article/16/2/338/246566">https://academic.oup.com/bib/article/16/2/338/246566</a>.
</div>
<div id="ref-breiman2001random" class="csl-entry">
Breiman, Leo. 2001a. <span>“Random Forests.”</span> <em>Machine Learning</em> 45 (1): 5–32.
</div>
<div id="ref-breiman2001modeling" class="csl-entry">
———. 2001b. <span>“Statistical Modeling: The Two Cultures.”</span> <em>Statistical Science</em> 16: 199–231. <a href="https://projecteuclid.org/euclid.ss/1009213726">https://projecteuclid.org/euclid.ss/1009213726</a>.
</div>
<div id="ref-gries2021statistics" class="csl-entry">
Gries, Stefan Th. 2021. <em>Statistics for Linguistics Using r: A Practical Introduction</em>. Berlin &amp; New York: Mouton de Gruyter.
</div>
<div id="ref-kursa2010feature" class="csl-entry">
Kursa, Miron B, Witold R Rudnicki, et al. 2010. <span>“Feature Selection with the Boruta Package.”</span> <em>Journal of Statistical Software</em> 36 (11): 1–13.
</div>
<div id="ref-prasad2006newer" class="csl-entry">
Prasad, Anantha M, Louis R Iverson, and Andy Liaw. 2006. <span>“Newer Classification and Regression Tree Techniques: Bagging and Random Forests for Ecological Prediction.”</span> <em>Ecosystems</em> 9 (2): 181–99.
</div>
<div id="ref-strobl2009tree" class="csl-entry">
Strobl, Carolin, James Malley, and Gerhard Tutz. 2009. <span>“An Introduction to Recursive Partitioning: Rationale,application and Characteristics of Classification and Regression Trees, Bagging and Random Forests.”</span> <em>Psychological Methods</em> 14 (4): 323–48. <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2927982/</a>.
</div>
<div id="ref-wrigt2016interac" class="csl-entry">
Wright, Marvin N., Andreas Ziegler, and Inke R. König. 2016. <span>“Do Little Interactions Get Lost in Dark Random Forests?”</span> 17 (145). <a href="https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-0995-8">https://bmcbioinformatics.biomedcentral.com/articles/10.1186/s12859-016-0995-8</a>.
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="working-with-tables.html" class="navigation navigation-prev navigation-unique" aria-label="Previous page"><i class="fa fa-angle-left"></i></a>

    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/MartinSchweinberger/acqvatabletree/edit/master/03-trees.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": "https://github.com/MartinSchweinberger/acqvatabletree/blob/master/03-trees.Rmd",
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
